{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Globe-LFMC samples and extract DEM Data\n",
    "Extracts the samples for locations in the CONUS from the Globe-LFMC spread sheet and adds the normalised DEM, climate zone, and other auxiliary data. DEM data is from the GEE SRTM DEM product, extracted using the MODIS projection and scale. Sites within the same MODIS pixel are merged. The following files are created:\n",
    "- `LFMC_CONUS.csv`: CONUS data extracted from the Globe-LFMC dataset\n",
    "- `LFMC_sites.csv`: sites extracted from the Globe-LFMC CONUS data and augmented with normalised DEM and location data\n",
    "- `LFMC_samples.csv`: Globe-LFMC CONUS sample data augmented with auxiliary variables\n",
    "\n",
    "### Notes\n",
    "1. The `Globe-LFMC-v2.xlsx` should exist in the `INPUT_DIR` directory - by default, a sub-directory of `DATA_DIR`\n",
    "2. The tiff containing the Koppen climate zone data (`Beck_KG_V1_present_0p0083.tif` available from https://figshare.com/articles/dataset/Present_and_future_K_ppen-Geiger_climate_classification_maps_at_1-km_resolution/6396959/2) should also be in `INPUT_DIR`, as should either the `legend.txt` file (available from the same site) or `Climate_zones.csv`. If `Climate_zones.csv` doesn't exist, it needs to be created from `legend.txt` by uncommenting and running the first code cell under \"Climate zone processing\".\n",
    "3. `EXTRACT_NAME` is a sub-directory of `DATA_DIR`. It will be created if it doesn't exist. All data files created by this and other data extraction notebooks will be located in sub-directories of this directory.\n",
    "4. `LFMC_CONUS.csv` is created in the `INPUT_DIR` directory.\n",
    "5. All other created files are CSVs and stored in the `SAMPLE_DIR` directory, by default a sub-directory of `DATA_DIR/EXTRACT_NAME`.\n",
    "6. The samples data output by this code is further processed by the MODIS extraction code to remove the snow samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import initialise\n",
    "import common\n",
    "from data_extract_utils import normalise_dem\n",
    "from data_extract_utils import extract_koppen_data\n",
    "from data_prep_utils import normalise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define input and output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globe-LFMC file and sheet name\n",
    "GLOBE_LFMC = os.path.join(common.SOURCE_DIR, \"Globe-LFMC-v2.xlsx\")\n",
    "SHEET_NAME = \"LFMC data\"\n",
    "\n",
    "# File Names\n",
    "LFMC_RAW = os.path.join(common.SOURCE_DIR, \"LFMC_CONUS.csv\")                   # CSV of CONUS data extracted from the Globe-LFMC dataset\n",
    "KOPPEN_TIF = os.path.join(common.SOURCE_DIR, 'Beck_KG_V1_present_0p0083.tif')  # Tiff of Koppen climate zone values\n",
    "LEGEND_FILE = os.path.join(common.SOURCE_DIR, 'legend.txt')                    # Text file with Koppen climate zone legend\n",
    "KOPPEN_LEGEND = os.path.join(common.SOURCE_DIR, 'Climate_zones.csv')           # CSV of Koppen climate zone legend\n",
    "\n",
    "if not os.path.exists(common.DATASETS_DIR):\n",
    "    os.makedirs(common.DATASETS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other constants/parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEM Product, projection and resolution\n",
    "DEM_PRODUCT = 'USGS/SRTMGL1_003'\n",
    "DEM_PROJ = \"EPSG:4326\"\n",
    "DEM_SCALE = 30\n",
    "\n",
    "# Floating point precision\n",
    "FLOAT_PRE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-based Processing\n",
    "Extracts the DEM data from GEE usong the native DEM projection and resolution. Keeps the sample site latitude and longitude, and adds the elevation/slope/aspect.\n",
    "- Parameter:\n",
    " - sites: Dataframe of sample sites\n",
    "- Returns: Dataframe of sites, latitude and longitude and the added elevation/slope/aspect attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sites_by_point(sites):\n",
    "    dem_image = ee.Terrain.products(ee.Image(DEM_PRODUCT))\n",
    "    points = [ee.Geometry.Point(site.Longitude, site.Latitude) for x, site in sites.iterrows()]\n",
    "    dem_col = ee.ImageCollection(dem_image)\n",
    "    col_list = [dem_col.getRegion(point, DEM_SCALE, DEM_PROJ) for point in points]\n",
    "    dem_list = ee.List(col_list).getInfo()\n",
    "    dem_data = pd.DataFrame([item[1] for item in dem_list], columns=dem_list[0][0])\n",
    "    dem_data.id = sites.Site\n",
    "    dem_data.rename(columns={\"id\": \"Site\"}, inplace=True)\n",
    "    dem_df = sites.merge(dem_data[['Site', 'elevation', 'slope', 'aspect']])\n",
    "    dem_df.columns = ['Site', 'Latitude', 'Longitude', 'Elevation', 'Slope', 'Aspect']\n",
    "    return dem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel-based Processing\n",
    "Extracts the DEM data at the requested projection and resolution. Terrain.products adds the slope and aspect. A reducer is used so terrain product info is added before resampling.\n",
    "- Parameters:\n",
    " - sites: dataframe of sampling sites\n",
    " - scale/proj: the required scale/proj (e.g. MODIS scale/proj - or map scale/proj)\n",
    " - maxPixels: Reducer parameter specifying the maximum number of DEM pixels to use to compute each down-sampled pixel. Doesn't need to be exact but make sure it's large enough - 512 is good for MODIS\n",
    "- Returns: Dataframe of sites with latitude and longitude set to the pixel centroid as returned by GEE and the added elevation/slope/aspect attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sites_by_pixel(sites, scale, proj, maxPixels):\n",
    "    dem_image = ee.Terrain.products(ee.Image(DEM_PRODUCT)).reduceResolution(ee.Reducer.mean(), maxPixels=maxPixels)\n",
    "    points = [ee.Geometry.Point(site.Longitude, site.Latitude) for x, site in sites.iterrows()]\n",
    "    dem_col = ee.ImageCollection(dem_image)\n",
    "    col_list = [dem_col.getRegion(point, scale, proj) for point in points]\n",
    "    dem_list = ee.List(col_list).getInfo()\n",
    "    dem_data = pd.DataFrame([item[1] for item in dem_list], columns=dem_list[0][0])\n",
    "    dem_data.id = sites.Site\n",
    "    dem_data.columns = ['Site', 'Longitude', 'Latitude', 'time', 'Elevation', 'Slope', 'Aspect', 'hillshade']\n",
    "    dem_df = dem_data.drop(columns=[\"time\", \"hillshade\"])\n",
    "    return dem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Processing\n",
    "- If the LFMC_RAW file already exists, load it.\n",
    "- Otherwise extract Globe LFMC data from the excel workbook sheet and save to the LFMC_RAW file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(LFMC_RAW):\n",
    "    LFMC_data = pd.read_csv(LFMC_RAW, index_col=0, float_precision=\"high\", parse_dates=[\"Sampling date\"],\n",
    "                           dtype={8: str, 10: np.int32, 11: np.int16, 14: np.int16, 23: str})\n",
    "else:    \n",
    "    LFMC_data = pd.read_excel(GLOBE_LFMC, SHEET_NAME).dropna(how=\"all\")\n",
    "    LFMC_data = LFMC_data[(LFMC_data.Country == \"USA\")\n",
    "                          & (LFMC_data[\"State/Region\"] != \"Alaska\")\n",
    "                          & (LFMC_data[\"Sampling date\"] >= common.START_DATE)]\n",
    "    LFMC_data.to_csv(LFMC_RAW)\n",
    "    LFMC_data = LFMC_data.astype(dtype={'Sampling year': np.int32, 'Protocol': np.int16, 'Units': np.int16})\n",
    "LFMC_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Site processing\n",
    "Extract the unique sites from the Globe-LFMC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "LFMC_data[\"Site\"] = LFMC_data.ID.str.rsplit(\"_\", 1, expand=True)[0]\n",
    "sites = LFMC_data[[\"Site\", \"Latitude\", \"Longitude\"]].drop_duplicates().reset_index(drop=True)\n",
    "sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the DEM data from GEE - run either sitesByPixel (pixel mode) or sitesByPoint (point mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dem_df = sites_by_pixel(sites, common.SCALE, common.PROJ, 512)\n",
    "dem_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise the DEM data and save the sites data. Note: sites with same latitude/longitude are *not* merged yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_norm = normalise_dem(dem_df.set_index('Site'), input_columns=['Longitude', 'Latitude', 'Elevation', 'Slope', 'Aspect'], precision=FLOAT_PRE)\n",
    "dem_norm = dem_norm.reset_index()\n",
    "dem_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date processing\n",
    "Create dataframe with dates and normalised day-of-year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "days = pd.date_range(common.START_DATE, common.END_DATE, closed=\"left\")\n",
    "doy = pd.Series(normalise(days.dayofyear, method='range', data_range=(1, 366), scaled_range=(-np.pi, np.pi)))\n",
    "days_df = pd.DataFrame({\"Date\": days, \n",
    "                        \"Day_sin\": doy.transform(np.sin).round(FLOAT_PRE),\n",
    "                        \"Day_cos\": doy.transform(np.cos).round(FLOAT_PRE)})\n",
    "days_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climate zone processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Koppen legend csv file\n",
    "If the `KOPPEN_LEGEND` file doesn't exist, uncomment and run the following cell. This will create it from the `legend.txt` file that can be downloaded with the climate zones tiff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# legend = {}\n",
    "# count = 0\n",
    "# with open(LEGEND_FILE) as fp:\n",
    "#     for ln in fp:\n",
    "#         line = ln.split(':')\n",
    "#         number = line[0].strip()\n",
    "#         if number.isnumeric():\n",
    "#             count += 1\n",
    "#             key = int(line[0].strip())\n",
    "#             parts = line[1].split('[')\n",
    "#             colour = parts[1].strip().strip(']').split(' ')\n",
    "#             code = parts[0].strip()[:3]\n",
    "#             descr = parts[0].strip()[5:]\n",
    "#             value = {'Number': number, 'Code': code, 'Description': descr, 'Red': colour[0], 'Green': colour[1], 'Blue': colour[2]}\n",
    "#             legend[key] = value\n",
    "# legend_df = pd.DataFrame.from_dict(legend, orient='index')\n",
    "# legend_df.to_csv(KOPPEN_LEGEND, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract climate zones for sites\n",
    "Extract the climate zone for each site and add to the sites data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cz_columns = ['Czone1', 'Czone2', 'Czone3']\n",
    "extract_koppen_data(KOPPEN_TIF, KOPPEN_LEGEND, sites, loc_columns=['Longitude', 'Latitude'], cz_columns=cz_columns)\n",
    "dem_norm = dem_norm.merge(sites[['Site', 'Czone1', 'Czone2', 'Czone3']], on='Site')\n",
    "dem_norm.to_csv(common.LFMC_SITES, index=False)\n",
    "dem_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample processing\n",
    "Create the auxiliary dataset from the samples\n",
    "\n",
    "##### Step 1: Merge sites and sample data to add the site longitude and latitude to the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dem_norm[[\"Site\", \"Longitude\", \"Latitude\"]].merge(\n",
    "    LFMC_data[[\"ID\", \"Site\", \"Sampling date\", \"Sampling year\", \"Land Cover\", \"LFMC value\"]])\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Merge samples for same latitude/longitude/date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a common site id for each site with the same latitude and longitude\n",
    "merge_columns = [\"Latitude\", \"Longitude\"]\n",
    "sites_temp = dem_norm[merge_columns + [\"Site\"]].groupby(merge_columns, as_index=False).min()\n",
    "# Merge samples for same year and location\n",
    "samples = samples.merge(sites_temp, on=merge_columns, suffixes=(\"_x\", None))\n",
    "groupby_cols = [\"Latitude\", \"Longitude\", \"Sampling date\"]\n",
    "data_cols = {\"ID\": \"min\",                                    # Unique sample ID is the first ID of the merged samples\n",
    "             \"Sampling year\": \"min\",                         # They should all be the same, but need to select one\n",
    "             \"Land Cover\": lambda x: pd.Series.mode(x)[0],   # Most common land cover value\n",
    "             \"LFMC value\": \"mean\",                           # mean LFMC value\n",
    "             \"Site\": \"min\"}                                  # Site id from sites_temp\n",
    "samples = samples[groupby_cols + list(data_cols.keys())].groupby(groupby_cols, as_index=False).\\\n",
    "              agg(data_cols).round({\"LFMC value\": FLOAT_PRE})\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Add the normalised auxiliary variables (day-of-year, location and DEM) to the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aux_df = samples[[\"ID\", \"Latitude\", \"Longitude\", \"Sampling date\", \"Sampling year\", \"Land Cover\", \"LFMC value\", \"Site\"]\n",
    "                ].merge(days_df, left_on=\"Sampling date\", right_on = \"Date\").drop(columns=\"Date\").\\\n",
    "                merge(dem_norm.drop(columns=[\"Longitude\", \"Latitude\"]), on=\"Site\").sort_values(\"ID\")\n",
    "aux_df = aux_df[['ID', 'Latitude', 'Longitude', 'Sampling date', 'Sampling year', 'Land Cover', 'LFMC value', 'Site',\n",
    "                 'Czone1', 'Czone2', 'Czone3',\n",
    "                 'Day_sin', 'Day_cos',\n",
    "                 'Long_sin', 'Long_cos', 'Lat_norm', 'Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos']]\n",
    "aux_df.to_csv(common.LFMC_SAMPLES, index=False)\n",
    "aux_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFMC",
   "language": "python",
   "name": "lfmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
