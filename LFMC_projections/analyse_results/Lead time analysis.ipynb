{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "progressive-angel",
   "metadata": {},
   "source": [
    "# Main Result\n",
    "This version removes validation samples from sites with no training data from the within-site results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dac64e-43f4-495f-a2a0-1b107022f2de",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerTuple, HandlerPatch\n",
    "import matplotlib.text as mpl_text\n",
    "from matplotlib.patches import ConnectionPatch, Rectangle\n",
    "\n",
    "import initialise\n",
    "import common\n",
    "from analysis_utils import calc_statistics, grouped_results\n",
    "from display_utils import display_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd885d30-0f93-40b9-8807-38379a1d40b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_file = os.path.join(common.DATASETS_DIR, 'samples_730days.csv')\n",
    "model_dir = os.path.join(common.MODELS_DIR, 'evaluation_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = common.ANALYSIS_MODEL\n",
    "ensemble_size = common.ENSEMBLE_SIZE\n",
    "ensemble_runs = common.ENSEMBLE_RUNS\n",
    "precision = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-junior",
   "metadata": {},
   "source": [
    "## Get the samples and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30db8fa5-1c75-4616-a5f4-3ddd96ea93d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_file_list(file_list, basename_prefix):\n",
    "    return sorted(\n",
    "        file_list,\n",
    "        key=lambda x: int(os.path.splitext(x)[0].rsplit(basename_prefix, 1)[1] or 0)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e18d81-212f-42de-b3d3-88fb3ebdcef3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stats_list = []\n",
    "test_list = []\n",
    "num_samples = []\n",
    "num_predicts = []\n",
    "samples = pd.read_csv(samples_file, index_col=0)\n",
    "with open(os.path.join(model_dir, 'experiment.json'), 'r') as f:\n",
    "    experiment = json.load(f)\n",
    "test_dirs = sort_file_list(glob.glob(os.path.join(model_dir, f'test*')), 'test')\n",
    "for test_num, test_dir in enumerate(test_dirs):\n",
    "    stats_ = pd.read_csv(os.path.join(test_dir, f'ensemble{ensemble_size}_stats.csv'), index_col=(0,1))\n",
    "    stats_list.append(stats_.loc[model].mean(axis=1))\n",
    "    try:\n",
    "        tn = experiment['tests'][test_num].get('testName') or experiment['testNames'][test_num]\n",
    "    except:\n",
    "        tn = f'Test {test_num}'\n",
    "    test_list.append(tn)\n",
    "    num_samples.append(samples.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdd69d-b95d-4411-9e0d-e0257e8397a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(stats_list, index=test_list)\n",
    "stats_df['#Samples'] = num_samples\n",
    "stats_df.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3551178-cfcf-49d1-8727-8cdae44263a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'model_params.json'), 'r') as f:\n",
    "    params = json.load(f)\n",
    "predicts = []\n",
    "for test_num, test_dir in enumerate(test_dirs):\n",
    "    predicts.append(pd.read_csv(os.path.join(test_dir, f'ensemble{ensemble_size}_{model}.csv'), index_col=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094477a9-0b11-4b79-8897-bc84b5d7ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples1 = samples.reindex(predicts[0].index)\n",
    "yearly_summ = samples1.groupby(['Sampling year']).agg(**{\n",
    "    \"Sample count\": pd.NamedAgg(column=params['targetColumn'], aggfunc=\"count\"),\n",
    "    \"LFMC minimum\": pd.NamedAgg(column=params['targetColumn'], aggfunc=\"min\"),\n",
    "    \"LFMC median\": pd.NamedAgg(column=params['targetColumn'], aggfunc=\"median\"),\n",
    "    \"LFMC mean\": pd.NamedAgg(column=params['targetColumn'], aggfunc=\"mean\"),\n",
    "    \"LFMC maximum\": pd.NamedAgg(column=params['targetColumn'], aggfunc=\"max\"),\n",
    "    \"LFMC std\": pd.NamedAgg(column=params['targetColumn'], aggfunc=\"std\"),\n",
    "}).round(2)\n",
    "\n",
    "rmse_df = grouped_results(samples1, predicts, 'Sampling year', params['targetColumn'], test_list, measures='RMSE')\n",
    "rmse_df = pd.concat([stats_df.RMSE, rmse_df], axis=1)\n",
    "rmse_df.rename(columns={'RMSE': 'All'}, inplace=True)\n",
    "\n",
    "r2_df = grouped_results(samples1, predicts, 'Sampling year', params['targetColumn'], test_list, measures='R2')\n",
    "r2_df = pd.concat([stats_df.R2, r2_df], axis=1)\n",
    "r2_df.rename(columns={'R2': 'All'}, inplace=True)\n",
    "\n",
    "bias_df = grouped_results(samples1, predicts, 'Sampling year', params['targetColumn'], test_list, measures='Bias')\n",
    "bias_df = pd.concat([stats_df.Bias, bias_df], axis=1)\n",
    "bias_df.rename(columns={'Bias': 'All'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c458ee-c452-4975-826d-8102f1b8691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_stats = [samples[params['targetColumn']].describe(), samples1[params['targetColumn']].describe()]\n",
    "for test_ in predicts:\n",
    "    summ_stats.append(test_.mean(axis=1).describe())\n",
    "summ_stats = pd.DataFrame(summ_stats, index=['All samples', 'Evaluation samples'] + test_list,\n",
    "                          columns=['count', 'min', '50%', 'mean', 'max', 'std']).round(1)\n",
    "summ_stats.columns = ['count', 'min', 'median', 'mean', 'max', 'std']\n",
    "summ_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8289bc7-9474-41fc-822c-add89b8ab3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of samples with LFMC <= 40:\",\n",
    "      samples1[samples1[params['targetColumn']] <= 40][params['targetColumn']].count())\n",
    "print(\"Number of samples with LFMC >= 255:\",\n",
    "      samples1[samples1[params['targetColumn']] >= 255][params['targetColumn']].count())\n",
    "print(\"Kurtosis of all evaluation samples:\",\n",
    "      samples1[params['targetColumn']].kurt().round(2))\n",
    "print(\"Kurtosis where LFMC > 40:\",\n",
    "      samples1[samples1[params['targetColumn']] > 40][params['targetColumn']].kurt().round(2))\n",
    "print(\"Kurtosis where LFMC < 255:\",\n",
    "      samples1[samples1[params['targetColumn']] < 255][params['targetColumn']].kurt().round(2))\n",
    "print(\"Kurtosis where LFMC between 40 and 255:\",\n",
    "      samples1[(samples1[params['targetColumn']] > 40) & (samples1[params['targetColumn']] < 255)]\n",
    "              [params['targetColumn']].kurt().round(2))\n",
    "print(\"Therefore, excess kurtosis appears to be due to the samples with very high LFMC.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306e984e-8549-4985-8feb-6b3a89d404f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyObject(object):\n",
    "    def __init__(self, text1, text2):\n",
    "        self.text1 = text1\n",
    "        self.text2 = text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180525e9-7b76-498b-be13-7e12267bd506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyObjectHandler(object):\n",
    "    def legend_artist(self, legend, text, fontsize, handlebox):\n",
    "        patch1 = mpl_text.Text(x=0, y=0, text=f'{text.text1} {text.text2}', size=5)\n",
    "        handlebox.add_artist(patch1)\n",
    "        return patch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbdc4b-1145-4e98-8539-e9d2ce947872",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [sns.color_palette()[0], sns.color_palette()[2], sns.color_palette()[1], sns.color_palette()[4], sns.color_palette()[5]]\n",
    "ls = ['-', '-.', '--', ':', (0, (4, 3, 1, 3, 1, 3))]\n",
    "marker = ['X', 'd', 'o', '*', 'P']\n",
    "cycler = plt.cycler(linestyle=ls, color=palette, marker=marker)\n",
    "font_size = 7\n",
    "title_size = 8\n",
    "plt.rcParams.update({'font.size': font_size})\n",
    "plt.rcParams.update({'font.sans-serif': 'Arial'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca0a1fb-b150-4af8-a8b2-4810e5e2f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summ_stats(summ_stats, ax, xlim=[None], ylim=[None], legend=True):\n",
    "    df = summ_stats.iloc[2:]\n",
    "    ax.set_box_aspect(1)\n",
    "    ax.set_prop_cycle(cycler)\n",
    "    ax = df[['max', 'mean', 'median', 'min']].plot.line(ax=ax, linewidth=1, markersize=4)\n",
    "    fill1 = ax.fill_between(\n",
    "        np.arange(13),\n",
    "        df['mean'] - df['std'],  #stats_plot['iqr_lower'],\n",
    "        df['mean'] + df['std'],  #stats_plot['iqr_upper'],\n",
    "        color=palette[1],\n",
    "        alpha=0.3, lw=0\n",
    "    )\n",
    "\n",
    "    mn = summ_stats.loc['Evaluation samples', 'mean']\n",
    "    sd = summ_stats.loc['Evaluation samples', 'std']\n",
    "    p5 = ax.plot([-1, -1], [mn-sd, mn+sd], color=palette[1], alpha=0.3, ls='-', lw=2, marker=\"None\")\n",
    "    p1 = ax.plot(-1, summ_stats.loc['Evaluation samples', 'max'], marker[0], markeredgewidth=0.7, markersize=4, markeredgecolor=palette[0], markerfacecolor='w')\n",
    "    p2 = ax.plot(-1, summ_stats.loc['Evaluation samples', 'mean'], marker[1], markeredgewidth=0.7, markersize=4, markeredgecolor=palette[1], markerfacecolor='None')\n",
    "    p3 = ax.plot(-1, summ_stats.loc['Evaluation samples', 'median'], marker[2], markeredgewidth=0.7, markersize=4, markeredgecolor=palette[2], markerfacecolor='None')\n",
    "    p4 = ax.plot(-1, summ_stats.loc['Evaluation samples', 'min'], marker[3], markeredgewidth=0.7, markersize=4, markeredgecolor=palette[3], markerfacecolor='w')\n",
    "\n",
    "    ax.set_xlabel('Lead time (months)', size=7)\n",
    "    ax.set_xticks(np.arange(13))\n",
    "    ax.set_xticklabels(np.arange(13), fontsize=7)\n",
    "    ax.set_xlim(*xlim)\n",
    "    ax.set_ylim(*ylim)\n",
    "    ax.tick_params(labelsize=7)\n",
    "    \n",
    "    # Add the legend\n",
    "    if legend:\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        obj_0 = AnyObject(\"Measured\", \"Estimated\")\n",
    "        temp_leg = ax.legend([obj_0] + list(zip([p1[0], p2[0], p3[0], p4[0]], handles)), [''] + labels,\n",
    "                             handler_map={obj_0:AnyObjectHandler(), tuple: HandlerTuple(ndivide=None)},\n",
    "                             fontsize=7, handlelength=6, labelspacing=0.3, borderpad=0.3, handletextpad=0.3)\n",
    "    else:\n",
    "        ax.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab85f789-3464-4081-868f-e7febe824f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_annotation(ax, text, point, text_point):\n",
    "    text_size = 5\n",
    "    text_box = dict(boxstyle=\"square, pad=0\", lw=0, fc='none')\n",
    "    arrow = {'arrowstyle': '->'}\n",
    "    ax.annotate(text, point, text_point, textcoords='offset points', size=text_size, arrowprops=arrow, bbox=text_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa0515-2a68-462e-9588-03bf7c2d1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_dfs = []\n",
    "for year in [2014, 2015, 2016, 2017]:\n",
    "    df_ = [samples1[samples1['Sampling year'] == year]['LFMC value'].describe()]\n",
    "    for test_ in predicts:\n",
    "        df_.append(test_[samples1['Sampling year'] == year].mean(axis=1).describe())\n",
    "    df_ = pd.DataFrame(df_, index=['Measured'] + test_list, columns=['count', 'min', '50%', 'mean', 'max', 'std']).round(1)\n",
    "    df_.columns = ['count', 'min', 'median', 'mean', 'max', 'std']\n",
    "    summ_dfs.append(df_)\n",
    "display_frames(summ_dfs, ['2014', '2015', '2016', '2017'], precision=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bc082d-63ef-4fbe-bf1e-7ee2ff1e5dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = summ_stats.loc[['Nowcasting', '3-months lead time', 'Evaluation samples']].rename(index={'Evaluation samples': 'Measured'})\n",
    "summ_df = pd.concat([temp_df] + [s.loc[['Nowcasting', '3-months lead time', 'Measured']] for s in summ_dfs],\n",
    "                    keys=['All years', '2014', '2015', '2016', '2017'])\n",
    "bp_data = [samples1[['Sampling year', 'LFMC value']].rename(columns={'LFMC value': 'LFMC (%)'})]\n",
    "bp_data.append(pd.concat([samples1['Sampling year'], predicts[0].mean(axis=1)], axis=1).rename(columns={0: 'LFMC (%)'}))\n",
    "bp_data.append(pd.concat([samples1['Sampling year'], predicts[3].mean(axis=1)], axis=1).rename(columns={0: 'LFMC (%)'}))\n",
    "bp_data = pd.concat(bp_data, keys=['Nowcast estimate', '3-month projection', 'Measured LFMC']).reset_index()\n",
    "bp_data1 = bp_data.copy()\n",
    "bp_data1['Sampling year'] = 'All years'\n",
    "bp_data = pd.concat([bp_data, bp_data1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c3bf29-d0ad-4e63-bd99-ec00a9281d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frames([rmse_df, r2_df, bias_df],\n",
    "               ['RMSE', 'R2', 'Bias'],\n",
    "               precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf603d0a-f2d9-438a-9077-9f113f89b0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = [sns.color_palette()[0], sns.color_palette()[1], sns.color_palette()[2], sns.color_palette()[4], sns.color_palette()[5]]\n",
    "ls = ['-', '-.', '--', ':', (0, (4, 3, 1, 3, 1, 3))]\n",
    "cycler = plt.cycler(linestyle=ls, color=palette, marker=['X', 'o', 'd', '*', 'P'])\n",
    "font_size = 7\n",
    "title_size = 8\n",
    "plt.rcParams.update({'font.size': font_size})\n",
    "plt.rcParams.update({'font.sans-serif': 'Arial'})\n",
    "fig, ((ax1, ax4), (ax2, ax5), (ax3, ax6)) = plt.subplots(3, 2, figsize=(5, 6), gridspec_kw={'width_ratios': [3, 1]},\n",
    "    sharex=True, sharey=False, constrained_layout=True, dpi=600, linewidth=2, edgecolor=\"black\")\n",
    "\n",
    "ax1.set_prop_cycle(cycler)\n",
    "ax1 = rmse_df.plot.line(ax=ax1, linewidth=1, markersize=4)\n",
    "ax1.set_xlabel('Lead time (months)', size=title_size)\n",
    "ax1.set_ylabel('RMSE (%)', size=title_size)\n",
    "ax1.set_xticks(np.arange(13))\n",
    "ax1.set_xticklabels(np.arange(13), fontsize=font_size)\n",
    "ax1.tick_params(labelsize=font_size)\n",
    "ax1.text(-1.8, 29.9, '(a)', va='top', ha='right', fontsize=title_size)\n",
    "\n",
    "ax2.set_prop_cycle(cycler)\n",
    "ax2 = r2_df.plot(ax=ax2, linewidth=1, markersize=4)\n",
    "ax2.set_xlabel('Lead time (months)', size=title_size)\n",
    "ax2.set_ylabel('$R^2$', size=title_size)\n",
    "ax2.set_xticks(np.arange(13))\n",
    "ax2.set_xticklabels(np.arange(13))\n",
    "ax2.tick_params(labelsize=font_size)\n",
    "ax2.set_ylim((0.3825, 0.5625))\n",
    "ax2.set_yticks(np.arange(40, 55, 2)/100)\n",
    "ax2.text(-1.8, 0.5625, '(b)', va='top', ha='right', fontsize=title_size)\n",
    "\n",
    "ax3.set_prop_cycle(cycler)\n",
    "ax3 = bias_df.plot(ax=ax3, linewidth=1, markersize=4)\n",
    "ax3.set_xlabel('Lead time (months)', size=title_size)\n",
    "ax3.set_ylabel('Bias (%)', size=title_size)\n",
    "ax3.set_xticks(np.arange(13))\n",
    "ax3.set_ylim((-4.7, 7.99))\n",
    "ax3.tick_params(labelsize=font_size)\n",
    "ax3.set_xticklabels(np.arange(13))\n",
    "ax3.text(-1.8, 8, '(c)', va='top', ha='right', fontsize=title_size)\n",
    "\n",
    "lines, labels = ax3.get_legend_handles_labels()\n",
    "ax4.axis('off')\n",
    "ax5.axis('off')\n",
    "ax6.axis('off')\n",
    "ax1.get_legend().remove()\n",
    "ax2.get_legend().remove()\n",
    "ax3.get_legend().remove()\n",
    "ax4.legend(lines, labels, loc = 'center', title='Legend', fontsize=font_size, handlelength=5, labelspacing=1.5, borderpad=1) #, borderaxespad=0.4)\n",
    "\n",
    "fig.savefig(os.path.join(common.FIGURES_DIR, 'Lead times.jpeg'), format=\"jpg\", bbox_inches='tight', pad_inches=0.2, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030ac92-f520-4733-9baa-e4c4b88ba0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['lines.linewidth'] = 0.5\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 3.5), dpi=200, linewidth=2, edgecolor=\"black\")\n",
    "\n",
    "ax = ax1\n",
    "ax = sns.boxplot(x='LFMC (%)', y='Sampling year', hue='level_0', data=bp_data, showfliers=False, linewidth=1, ax=ax, orient='h',\n",
    "                 order=[2014, 2015, 2016, 2017, 'All years'],\n",
    "                 hue_order=['Nowcast estimate', '3-month projection', 'Measured LFMC'])\n",
    "\n",
    "for xtick in ax.get_yticks():\n",
    "    year_ = ax.get_yticklabels()[xtick].get_text()\n",
    "    for pos, label in enumerate(['Nowcasting', '3-months lead time', 'Measured', ]):\n",
    "        x_off = xtick + [-0.267, 0.0, .267][pos]\n",
    "        \n",
    "        # Add the minimums and maximums\n",
    "        min_ = summ_df.loc[(year_, label), 'min']\n",
    "        max_ = summ_df.loc[(year_, label), 'max']\n",
    "        col_ = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"][pos]\n",
    "        pt, = ax.plot(min_, x_off, 'ok', markersize=5, scalex=False,\n",
    "                      mfc=col_, mec=col_)\n",
    "        pt, = ax.plot(max_, x_off, '*k', markersize=7, scalex=False,\n",
    "                      mfc=col_, mec=col_)\n",
    "\n",
    "ax.set_ylabel('Evaluation year')\n",
    "ax.set_yticklabels([2014, 2015, 2016, 2017, 'All  \\nyears']) #, ha='center')\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(0.99, 0.995), labelspacing=0.3, borderpad=0.4, handletextpad=0.3, borderaxespad=0)\n",
    "ax.set_xlim(-10, 450)\n",
    "\n",
    "ax = ax2\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "tab_font_size = 7\n",
    "tab_col_widths = [0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "tab_line_width = 0.5\n",
    "row_names = dict(zip(['Nowcasting', '3-month lead time', 'Measured'], ['Nowcast estimate', '3-month projection', 'Measured LFMC']))\n",
    "\n",
    "temp_df = summ_df.loc['2014'].drop(columns='count').round(1).rename(row_names)\n",
    "count_ = int(summ_df.loc['2014', 'count'][0])\n",
    "tab1 = ax.table(cellText=temp_df.values, rowLabels=temp_df.index, colLabels=temp_df.columns,\n",
    "                colWidths=tab_col_widths, bbox=[0.25, .816, 0.75, .184])\n",
    "width_ = tab1[1, -1].get_width()\n",
    "height_ = tab1[0, 0].get_height()\n",
    "tab1.add_cell(0, -1, width=width_, height=height_, text=f'2014 (count: {count_})', fontproperties=dict(size=tab_font_size))\n",
    "tab1.auto_set_font_size(False)\n",
    "tab1.set_fontsize(tab_font_size)\n",
    "for cell_ in tab1.get_children():\n",
    "    cell_._linewidth = tab_line_width\n",
    "\n",
    "temp_df = summ_df.loc['2015'].drop(columns='count').rename(row_names)\n",
    "count_ = int(summ_df.loc['2015', 'count'][0])\n",
    "tab2 = ax.table(cellText=temp_df.values, rowLabels=temp_df.index, colLabels=temp_df.columns, \n",
    "                colWidths=tab_col_widths, bbox=[0.25, .612, 0.75, .184])\n",
    "width_ = tab2[1, -1].get_width()\n",
    "height_ = tab2[0, 0].get_height()\n",
    "tab2.add_cell(0, -1, width=width_, height=height_, text=f'2015 (count: {count_})', fontproperties=dict(size=tab_font_size))\n",
    "tab2.auto_set_font_size(False)\n",
    "tab2.set_fontsize(tab_font_size)\n",
    "for cell_ in tab2.get_children():\n",
    "    cell_._linewidth = tab_line_width\n",
    "\n",
    "temp_df = summ_df.loc['2016'].drop(columns='count').rename(row_names)\n",
    "count_ = int(summ_df.loc['2016', 'count'][0])\n",
    "tab3 = ax.table(cellText=temp_df.values, rowLabels=temp_df.index, colLabels=temp_df.columns,\n",
    "                colWidths=tab_col_widths, bbox=[0.25, .408, 0.75, .184])\n",
    "width_ = tab3[1, -1].get_width()\n",
    "height_ = tab3[0, 0].get_height()\n",
    "tab3.add_cell(0, -1, width=width_, height=height_, text=f'2016 (count: {count_})', fontproperties=dict(size=tab_font_size))\n",
    "tab3.auto_set_font_size(False)\n",
    "tab3.set_fontsize(tab_font_size)\n",
    "for cell_ in tab3.get_children():\n",
    "    cell_._linewidth = tab_line_width\n",
    "\n",
    "temp_df = summ_df.loc['2017'].drop(columns='count').rename(row_names)\n",
    "count_ = int(summ_df.loc['2017', 'count'][0])\n",
    "tab4 = ax.table(cellText=temp_df.values, rowLabels=temp_df.index, colLabels=temp_df.columns,\n",
    "                colWidths=tab_col_widths, bbox=[0.25, .204, 0.75, .184])\n",
    "width_ = tab4[1, -1].get_width()\n",
    "height_ = tab4[0, 0].get_height()\n",
    "tab4.add_cell(0, -1, width=width_, height=height_, text=f'2017 (count: {count_})', fontproperties=dict(size=tab_font_size))\n",
    "tab4.auto_set_font_size(False)\n",
    "tab4.set_fontsize(tab_font_size)\n",
    "for cell_ in tab4.get_children():\n",
    "    cell_._linewidth = tab_line_width\n",
    "\n",
    "temp_df = summ_df.loc['All years'].drop(columns='count').rename(row_names)\n",
    "count_ = int(summ_df.loc['All years', 'count'][0])\n",
    "tab5 = ax.table(cellText=temp_df.values, rowLabels=temp_df.index, colLabels=temp_df.columns,\n",
    "                colWidths=tab_col_widths, bbox=[0.25, 0, 0.75, .184])\n",
    "width_ = tab5[1, -1].get_width()\n",
    "height_ = tab5[0, 0].get_height()\n",
    "tab5.add_cell(0, -1, width=width_, height=height_, text=f'All (count: {count_})', fontproperties=dict(size=tab_font_size))\n",
    "tab5.auto_set_font_size(False)\n",
    "tab5.set_fontsize(tab_font_size)\n",
    "for cell_ in tab5.get_children():\n",
    "    cell_._linewidth = tab_line_width\n",
    "\n",
    "fig.savefig(os.path.join(common.FIGURES_DIR, 'Summary_stats.jpeg'), format=\"jpg\", bbox_inches='tight', pad_inches=0.1, dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce66857-4333-4c01-b144-95410f780117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFMC",
   "language": "python",
   "name": "lfmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
