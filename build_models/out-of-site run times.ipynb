{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-site run times\n",
    "Version of the out-of-site ablation tests that gets the run-times for the final model and architecture ablation tests.\n",
    "#### Notes\n",
    "- Only one fold model is created for each run\n",
    "- Only 20 runs per test - i.e. timings are for one ensemble of twenty runs.\n",
    "- No parallel running of tests\n",
    "- The first test (test0) is the proposed model, so other tests are offset by 1 compared to the full ablation test (e.g. the dropout test is test4 here and test3 in the ablation test)\n",
    "- By default, training times are output to the `train_stats.csv` file, so these could also be obtained from the ablation tests model directories. This notebook allows testing under more controlled conditions - e.g. if the full runs are run in a shared environment they may not be accurate/consistent due to the server workload. So this is a cut-down version that can run in a small dedicated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import initialise\n",
    "import common\n",
    "from model_utils import reshape_data\n",
    "from modelling_functions import create_models, run_experiment\n",
    "from architecture_out_of_site import model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input files\n",
    "Change these settings as required\n",
    "- `modis_csv`: The file containing extracted MODIS data for each sample, created by `Extract MODIS Data.ipynb`\n",
    "- `prism_csv`: The file containing extracted PRISM data for each sample, created by `Extract PRISM Data.ipynb`\n",
    "- `aux_csv`: The file containing extracted sample labels, DEM, climate zone and other auxiliary data, created by `Extract Auxiliary Data.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_csv = os.path.join(common.DATASETS_DIR, 'modis_365days.csv')\n",
    "prism_csv = os.path.join(common.DATASETS_DIR, 'prism_365days.csv')\n",
    "aux_csv = os.path.join(common.DATASETS_DIR, 'samples_365days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up experiment parameters\n",
    "If the experiment dictionary contains a 'tests' key that is not 'falsy' (False, None, 0, empty list) it is assumed to be a list of tests to run. Each test will run with the specified model parameters. Model parameters not specified will be the same for each test, as set in the main model_params dictionary. A failed run can be restarted by setting the 'restart' key to the test that failed. This test and the remaining tests will then be run.\n",
    "\n",
    "If 'tests' is 'falsy' then a single test will be run using the parameters in the main model_params dictionary.\n",
    "\n",
    "Other settings are:\n",
    "- layerTypes: specifies which layers to include in the model\n",
    "- Layer parameters should be specified as a list. The first entry in the list will be used for the first layer, etc.\n",
    "- If the experiment includes changes to the layers, all non-default layer parameters need to be included. The parameters that are kept constant can be specified by including a key for the layer type in the experiment dictionary, and the value set to a dictionary of the constant parameters.\n",
    "\n",
    "Model_parameters that cannot be changed in tests are:\n",
    "- \\*Filename\n",
    "- \\*Channels\n",
    "- targetColumn\n",
    "\n",
    "Example of setting layer parameters:  \n",
    "```\n",
    "{'name': 'Filters',  \n",
    " 'description': 'Test effect of different filter sizes on conv layers',  \n",
    " 'tests': [{'conv': {'filters': [32, 32, 32]}},  \n",
    "           {'conv': {'filters': [8, 8, 8]}},  \n",
    "           {'conv': {'filters': [32, 8, 8]}},  \n",
    "           {'conv': {'filters': [8, 32, 8]}},  \n",
    "           {'conv': {'filters': [8, 8, 32]}},  \n",
    "           {'conv': {'filters': [8, 16, 32]}},  \n",
    "           {'conv': {'filters': [32, 16, 8]}}],  \n",
    " 'conv': {'numLayers': 3, 'poolSize': [2, 3, 4]},  \n",
    " 'restart': 0}\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'out-of-site_timings',\n",
       " 'description': 'Timings for out-of-site architecture changes',\n",
       " 'layerTypes': ['modisConv', 'prismConv', 'fc'],\n",
       " 'tests': [{},\n",
       "  {'modisConv': {'numLayers': 3,\n",
       "    'filters': [32, 32, 32],\n",
       "    'poolSize': [2, 3, 4]},\n",
       "   'prismConv': {'numLayers': 3,\n",
       "    'filters': [32, 32, 32],\n",
       "    'poolSize': [2, 3, 4]}},\n",
       "  {'fc': {'numLayers': 2, 'units': [128, 128]}},\n",
       "  {'fc': {'numLayers': 1, 'units': [256]}},\n",
       "  {'dropoutRate': 0.5},\n",
       "  {'batchSize': 32},\n",
       "  {'epochs': 100}],\n",
       " 'testNames': ['Proposed model',\n",
       "  'Conv filters: 32',\n",
       "  'Dense layers: 2',\n",
       "  'Dense units: 256',\n",
       "  'Dropout: 0.5',\n",
       "  'Batch size: 32',\n",
       "  'Epochs: 100'],\n",
       " 'restart': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = {\n",
    "    'name': 'out-of-site_timings',\n",
    "    'description': 'Timings for out-of-site architecture changes',\n",
    "    'layerTypes': ['modisConv', 'prismConv', 'fc'],\n",
    "    'tests': [\n",
    "        {},\n",
    "        {'modisConv': {'numLayers': 3, 'filters': [32, 32, 32], 'poolSize': [2, 3, 4]},\n",
    "         'prismConv': {'numLayers': 3, 'filters': [32, 32, 32], 'poolSize': [2, 3, 4]}\n",
    "        },\n",
    "        {'fc': {'numLayers': 2, 'units': [128, 128]}},\n",
    "        {'fc': {'numLayers': 1, 'units': [256]}},\n",
    "        {'dropoutRate': 0.5},\n",
    "        {'batchSize': 32},\n",
    "        {'epochs': 100},\n",
    "    ],\n",
    "    'testNames': [\n",
    "        'Proposed model',\n",
    "        'Conv filters: 32',\n",
    "        'Dense layers: 2',\n",
    "        'Dense units: 256',\n",
    "        'Dropout: 0.5',\n",
    "        'Batch size: 32',\n",
    "        'Epochs: 100',\n",
    "    ],\n",
    "    'restart': None,\n",
    "}\n",
    "\n",
    "# Save and display experiment details\n",
    "experiment_dir = os.path.join(common.MODELS_DIR, experiment['name'])\n",
    "restart = experiment.get('restart')\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "elif restart is None:\n",
    "    raise FileExistsError(f'{experiment_dir} exists but restart not requested')\n",
    "experiment_file = f'experiment{restart}.json' if restart else 'experiment.json'\n",
    "with open(os.path.join(experiment_dir, experiment_file), 'w') as f:\n",
    "    json.dump(experiment, f, indent=2)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters settings\n",
    "To find out more about any parameter, run `model_params.help('<parameter>')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelName': 'out-of-site_timings',\n",
       " 'description': 'Timings for out-of-site architecture changes',\n",
       " 'modelClass': 'LfmcTempCnn',\n",
       " 'modelDir': 'G:\\\\My Drive\\\\LFMC Data\\\\multi_modal_LFMC\\\\Models\\\\out-of-site_timings',\n",
       " 'tempDir': 'C:\\\\Temp\\\\LFMC',\n",
       " 'diagnostics': False,\n",
       " 'dataSources': ['modis', 'prism', 'aux'],\n",
       " 'restartRun': None,\n",
       " 'derivedModels': {'merge10': {'type': 'merge', 'models': 10}},\n",
       " 'saveModels': False,\n",
       " 'saveTrain': None,\n",
       " 'saveValidation': True,\n",
       " 'plotModel': True,\n",
       " 'randomSeed': 1234,\n",
       " 'modelSeed': 1234,\n",
       " 'modelRuns': 3,\n",
       " 'resplit': False,\n",
       " 'seedList': [441,\n",
       "  780,\n",
       "  328,\n",
       "  718,\n",
       "  184,\n",
       "  372,\n",
       "  346,\n",
       "  363,\n",
       "  701,\n",
       "  358,\n",
       "  566,\n",
       "  451,\n",
       "  795,\n",
       "  237,\n",
       "  788,\n",
       "  185,\n",
       "  397,\n",
       "  530,\n",
       "  758,\n",
       "  633],\n",
       " 'maxWorkers': 1,\n",
       " 'deterministic': False,\n",
       " 'gpuDevice': 0,\n",
       " 'gpuMemory': 0,\n",
       " 'modisFilename': 'G:\\\\My Drive\\\\LFMC Data\\\\multi_modal_LFMC\\\\Datasets\\\\modis_365days.csv',\n",
       " 'modisChannels': 7,\n",
       " 'modisNormalise': {'method': 'minMax', 'percentiles': 2},\n",
       " 'prismFilename': 'G:\\\\My Drive\\\\LFMC Data\\\\multi_modal_LFMC\\\\Datasets\\\\prism_365days.csv',\n",
       " 'prismChannels': 7,\n",
       " 'prismNormalise': {'method': 'minMax', 'percentiles': 2},\n",
       " 'auxFilename': 'G:\\\\My Drive\\\\LFMC Data\\\\multi_modal_LFMC\\\\Datasets\\\\samples_365days.csv',\n",
       " 'auxColumns': ['Elevation',\n",
       "  'Slope',\n",
       "  'Aspect_sin',\n",
       "  'Aspect_cos',\n",
       "  'Long_sin',\n",
       "  'Long_cos',\n",
       "  'Lat_norm'],\n",
       " 'auxAugment': True,\n",
       " 'auxOneHotCols': ['Czone3'],\n",
       " 'targetColumn': 'LFMC value',\n",
       " 'splitMethod': 'bySite',\n",
       " 'splitSizes': (0.1, 0),\n",
       " 'siteColumn': 'Site',\n",
       " 'splitStratify': 'Land Cover',\n",
       " 'splitYear': None,\n",
       " 'yearColumn': 'Sampling year',\n",
       " 'splitFolds': 0,\n",
       " 'convPadding': 'same',\n",
       " 'poolPadding': 'valid',\n",
       " 'batchNormalise': True,\n",
       " 'dropoutRate': 0,\n",
       " 'regulariser': 'keras.regularizers.l2(1.e-6)',\n",
       " 'validationSet': False,\n",
       " 'earlyStopping': False,\n",
       " 'epochs': 10,\n",
       " 'evaluateEpochs': None,\n",
       " 'batchSize': 512,\n",
       " 'shuffle': True,\n",
       " 'verbose': 0,\n",
       " 'optimiser': 'adam',\n",
       " 'activation': 'relu',\n",
       " 'initialiser': 'he_normal',\n",
       " 'loss': 'mean_squared_error',\n",
       " 'metrics': ['mean_absolute_error'],\n",
       " 'modisConv': [{'filters': 8,\n",
       "   'kernel': 5,\n",
       "   'stride': 1,\n",
       "   'dilation': 1,\n",
       "   'bnorm': True,\n",
       "   'poolSize': 2},\n",
       "  {'filters': 8,\n",
       "   'kernel': 5,\n",
       "   'stride': 1,\n",
       "   'dilation': 1,\n",
       "   'bnorm': True,\n",
       "   'poolSize': 3},\n",
       "  {'filters': 8,\n",
       "   'kernel': 5,\n",
       "   'stride': 1,\n",
       "   'dilation': 1,\n",
       "   'bnorm': True,\n",
       "   'poolSize': 4}],\n",
       " 'prismConv': [{'filters': 8,\n",
       "   'kernel': 5,\n",
       "   'stride': 1,\n",
       "   'dilation': 1,\n",
       "   'bnorm': True,\n",
       "   'poolSize': 2},\n",
       "  {'filters': 8,\n",
       "   'kernel': 5,\n",
       "   'stride': 1,\n",
       "   'dilation': 1,\n",
       "   'bnorm': True,\n",
       "   'poolSize': 3},\n",
       "  {'filters': 8,\n",
       "   'kernel': 5,\n",
       "   'stride': 1,\n",
       "   'dilation': 1,\n",
       "   'bnorm': True,\n",
       "   'poolSize': 4}],\n",
       " 'fc': [{'units': 128, 'bnorm': True}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params['modelName'] = experiment['name']\n",
    "model_params['description'] = experiment['description']\n",
    "model_params['modisFilename'] = modis_csv\n",
    "model_params['prismFilename'] = prism_csv\n",
    "model_params['auxFilename'] = aux_csv\n",
    "model_params['splitMethod'] = 'bySite'\n",
    "model_params['splitFolds'] = 0\n",
    "model_params['splitSizes'] = (0.1, 0)\n",
    "model_params['tempDir'] = common.TEMP_DIR\n",
    "model_params['modelDir'] = os.path.join(common.MODELS_DIR, model_params['modelName'])\n",
    "model_params['derivedModels'] = common.DERIVED_MODELS\n",
    "model_params['seedList'] = [\n",
    "    441, 780, 328, 718, 184, 372, 346, 363, 701, 358,\n",
    "    566, 451, 795, 237, 788, 185, 397, 530, 758, 633,\n",
    "]\n",
    "\n",
    "if not os.path.exists(model_params['modelDir']):\n",
    "    os.makedirs(model_params['modelDir'])\n",
    "\n",
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modis shape: (5000, 365, 7)\n",
      "Prism shape: (5000, 365, 7)\n"
     ]
    }
   ],
   "source": [
    "modis_data = pd.read_csv(model_params['modisFilename'], index_col=0)\n",
    "x_modis = reshape_data(np.array(modis_data), model_params['modisChannels'])\n",
    "print(f'Modis shape: {x_modis.shape}')\n",
    "\n",
    "prism_data = pd.read_csv(model_params['prismFilename'], index_col=0)\n",
    "x_prism = reshape_data(np.array(prism_data), model_params['prismChannels'])\n",
    "print(f'Prism shape: {x_prism.shape}')\n",
    "\n",
    "aux_data = pd.read_csv(model_params['auxFilename'], index_col=0)\n",
    "y = aux_data[model_params['targetColumn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the models\n",
    "Builds and trains the LFMC models. After training each model, several derived models are created and evaluated. The full list of models is:\n",
    "- `base` - The fully trained model\n",
    "- `best` - A model using the checkpoint with the best training loss\n",
    "- `merge10` - A model created by merging the last 10 checkpoints. The checkpoints are merged by averaging the corresponding weights from each model.\n",
    "- `ensemble10` - An ensembled model of the last 10 checkpoints. This model averages the predictions made by each model in the ensemble to make the final prediction.\n",
    "- `merge_best10` - Similar to the merge10 model, but uses the 10 checkpoints with the lowest training/validation losses.\n",
    "\n",
    "All models, predictions, evaluation statistics, and plots of test results are saved to `model_dir`, with each test and run saved to a separate sub-directory. For each model created, predictions and evaluation statistics are also returned as attributes of the `model` object. These are stored as nested lists, the structure for a full experiment is:\n",
    "- Tests (omitted if not an experiment)\n",
    "  - Runs (omitted for a single run)\n",
    "    - Folds (for k-fold splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_experiment():\n",
    "    try:\n",
    "        return bool(experiment['tests'])\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================================================\n",
      "Experiment out-of-site_timings - Timings for out-of-site architecture changes\n",
      "=============================================================================\n",
      "\n",
      "Test Proposed model - {}\n",
      "\n",
      "Auxiliary columns: ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm']\n",
      "modis shape: (5000, 365, 7)\n",
      "prism shape: (5000, 365, 7)\n",
      "aux shape: (5000, 31)\n",
      "Split by Site, stratify by Land Cover\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test Conv filters: 32 - {'modisConv': {'numLayers': 3, 'filters': [32, 32, 32], 'poolSize': [2, 3, 4]}, 'prismConv': {'numLayers': 3, 'filters': [32, 32, 32], 'poolSize': [2, 3, 4]}}\n",
      "\n",
      "Auxiliary columns: ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm']\n",
      "modis shape: (5000, 365, 7)\n",
      "prism shape: (5000, 365, 7)\n",
      "aux shape: (5000, 31)\n",
      "Split by Site, stratify by Land Cover\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test Dense layers: 2 - {'fc': {'numLayers': 2, 'units': [128, 128]}}\n",
      "\n",
      "Auxiliary columns: ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm']\n",
      "modis shape: (5000, 365, 7)\n",
      "prism shape: (5000, 365, 7)\n",
      "aux shape: (5000, 31)\n",
      "Split by Site, stratify by Land Cover\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test Dense units: 256 - {'fc': {'numLayers': 1, 'units': [256]}}\n",
      "\n",
      "Auxiliary columns: ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm']\n",
      "modis shape: (5000, 365, 7)\n",
      "prism shape: (5000, 365, 7)\n",
      "aux shape: (5000, 31)\n",
      "Split by Site, stratify by Land Cover\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test Dropout: 0.5 - {'dropoutRate': 0.5}\n",
      "\n",
      "Auxiliary columns: ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm']\n",
      "modis shape: (5000, 365, 7)\n",
      "prism shape: (5000, 365, 7)\n",
      "aux shape: (5000, 31)\n",
      "Split by Site, stratify by Land Cover\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test Batch size: 32 - {'batchSize': 32}\n",
      "\n",
      "Auxiliary columns: ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm']\n",
      "modis shape: (5000, 365, 7)\n",
      "prism shape: (5000, 365, 7)\n",
      "aux shape: (5000, 31)\n",
      "Split by Site, stratify by Land Cover\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Test Epochs: 100 - {'epochs': 100}\n",
      "\n",
      "Auxiliary columns: ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm']\n",
      "modis shape: (5000, 365, 7)\n",
      "prism shape: (5000, 365, 7)\n",
      "aux shape: (5000, 31)\n",
      "Split by Site, stratify by Land Cover\n"
     ]
    }
   ],
   "source": [
    "X = {'modis': x_modis, 'prism': x_prism}\n",
    "if is_experiment():\n",
    "    ex_models = run_experiment(experiment, model_params, aux_data, X, y)\n",
    "else:\n",
    "    print('Running a single test')\n",
    "    with open(os.path.join(model_params['modelDir'], 'model_params.json'), 'w') as f:\n",
    "        model_params.save(f)\n",
    "    models = create_models(model_params, aux_data, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the training times\n",
    "Time are Tensorflow/Keras model training time and excludes data preparation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ensemble_time</th>\n",
       "      <th>single_time</th>\n",
       "      <th>num_params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Proposed model</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.10</td>\n",
       "      <td>37185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conv filters: 32</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.17</td>\n",
       "      <td>150657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense layers: 2</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.10</td>\n",
       "      <td>53953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dense units: 256</th>\n",
       "      <td>0.29</td>\n",
       "      <td>0.10</td>\n",
       "      <td>72385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dropout: 0.5</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.10</td>\n",
       "      <td>37185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Batch size: 32</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.20</td>\n",
       "      <td>37185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epochs: 100</th>\n",
       "      <td>1.81</td>\n",
       "      <td>0.60</td>\n",
       "      <td>37185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  ensemble_time  single_time  num_params\n",
       "Proposed model             0.29         0.10       37185\n",
       "Conv filters: 32           0.50         0.17      150657\n",
       "Dense layers: 2            0.30         0.10       53953\n",
       "Dense units: 256           0.29         0.10       72385\n",
       "Dropout: 0.5               0.31         0.10       37185\n",
       "Batch size: 32             0.59         0.20       37185\n",
       "Epochs: 100                1.81         0.60       37185"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_times = []\n",
    "for test in ex_models:\n",
    "    run_time = 0\n",
    "    for run in test:\n",
    "        df = pd.read_csv(os.path.join(run.model_dir, 'train_stats.csv'))\n",
    "        run_time += df.trainTime[0]\n",
    "    weights = df.trainableWeights[0]\n",
    "    train_times.append([run_time/60, run_time/60/len(test), weights])\n",
    "pd.DataFrame(train_times, index=experiment['testNames'],\n",
    "             columns=['ensemble_time', 'single_time', 'num_params']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFMC",
   "language": "python",
   "name": "lfmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
