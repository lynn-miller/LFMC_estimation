{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed595e42-4a77-4afc-8341-fc619e93ba22",
   "metadata": {},
   "source": [
    "# Create Ensembles for Comparison Models\n",
    "- Generates ensembles for both the within-site and out-of-site models\n",
    "- Ensemble size is 20\n",
    "- 50 ensembles are created by sampling (without replacement) from the pool of models\n",
    "- Save the ensemble predictions and statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4ce86-4017-4e7d-b46a-560c68ca84bb",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import initialise\n",
    "import common\n",
    "from analysis_utils import calc_statistics, samples_with_historical_data\n",
    "from model_utils import generate_ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f49a7b5-6bf8-4425-8d8f-e83599875850",
   "metadata": {},
   "source": [
    "## Directories and other settings\n",
    "- Update the model directories as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dacc714-5848-4666-983f-255e8e98cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_file = os.path.join(common.DATASETS_DIR, 'samples_365days.csv')\n",
    "model_dir = os.path.join(common.MODELS_DIR, 'comparison_models')\n",
    "\n",
    "ensemble_model = common.ANALYSIS_MODEL\n",
    "ensemble_size = common.ENSEMBLE_SIZE\n",
    "ensemble_runs = common.ENSEMBLE_RUNS\n",
    "\n",
    "precision = 3       # floating point precision for saved predictions\n",
    "random_seed = 9876\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Ensembles are needed for the Multi-tempCNN comparisons (tests 0 & 1)\n",
    "ensemble_range = range(2)\n",
    "\n",
    "# Single models are needed for the Modis-tempCNN comparisons (tests 2 & 3)  \n",
    "single_range = range(2, 4)\n",
    "single_model = common.MODIS_TEMPCNN_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809aa7ee-d102-4a8d-a4d2-15db2353c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(model_dir, 'model_params.json'), 'r') as f:\n",
    "    model_params = json.load(f)\n",
    "samples1 = pd.read_csv(samples_file, index_col=0)\n",
    "temp_predict = pd.read_csv(os.path.join(model_dir, 'test0', 'run0', 'predictions.csv'), index_col=0)\n",
    "samples2, _ = samples_with_historical_data(samples1, temp_predict, model_params['siteColumn'], model_params['yearColumn'])\n",
    "samples_index = [samples2.index, samples1.index, samples2.index, samples1.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-infrastructure",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Generate the ensembles\n",
    "Generate ensembles of various sizes for each of the models. For each test, randomly select the runs to ensemble, then create the ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef1671-96c3-4ece-afb0-ed631a6c92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predicts = []\n",
    "for i in ensemble_range:\n",
    "    test_predicts = []\n",
    "    for run_dir in glob.glob(os.path.join(model_dir, f'test{i}', 'run*')):\n",
    "        test_predicts.append(pd.read_csv(os.path.join(run_dir, 'predictions.csv'), index_col=0).loc[samples_index[i]])\n",
    "    model_predicts.append(test_predicts)\n",
    "num_models = len(model_predicts[0])\n",
    "\n",
    "predict, all_stats = generate_ensembles(model_predicts, common.ENSEMBLE_RUNS, common.ENSEMBLE_SIZE, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded852b-c065-4a16-8045-2ed00fda1aa8",
   "metadata": {},
   "source": [
    "### Save Ensemble Predictions\n",
    "Save the predictions made by the ensembles using the model of interest. Columns are the individual ensemble predictions and rows are the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190e0c8d-f762-42b4-9ae9-acc66bac56db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in ensemble_range:\n",
    "    file_name = f\"ensemble{ensemble_size}_{ensemble_model}.csv\"\n",
    "    stats_fname = f\"ensemble{ensemble_size}_stats.csv\"\n",
    "    test_name = f'test{num}'\n",
    "    print(os.path.join(model_dir, test_name, file_name))\n",
    "    print(os.path.join(model_dir, test_name, stats_fname))\n",
    "    df = pd.concat([pred_[common.ANALYSIS_MODEL] for pred_ in predict[num]], axis=1, ignore_index=True).round(precision)\n",
    "    df.to_csv(os.path.join(model_dir, test_name, file_name))\n",
    "    df = pd.DataFrame([run.stack() for run in all_stats[num]]).T\n",
    "    df.to_csv(os.path.join(model_dir, test_name, stats_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854d31d-639a-4643-9358-b900857c27c6",
   "metadata": {},
   "source": [
    "### Merge all model Predictions\n",
    "Merge the predictions from each run of the Modis-tempCNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89588f77-d40f-40d5-9932-4b7ae6613d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predicts = []\n",
    "for i in single_range:\n",
    "    test_predicts = []\n",
    "    for run_dir in glob.glob(os.path.join(model_dir, f'test{i}', 'run*')):\n",
    "        test_predicts.append(pd.read_csv(os.path.join(run_dir, 'predictions.csv'), index_col=0).loc[samples_index[i]])\n",
    "    model_predicts.append(test_predicts)\n",
    "num_models = len(model_predicts[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6445fe44-3c1b-4e40-9104-916d98c9a1a6",
   "metadata": {},
   "source": [
    "### Save Modis-tempCNN Predictions\n",
    "Save the predictions made by the Modis-tempCNN models. Columns are the individual model predictions and rows are the samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0ea0ca-e181-4e83-bea4-525086395125",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num in single_range:\n",
    "    file_name = f\"predictions_{single_model}.csv\"\n",
    "    stats_fname = \"predictions_stats.csv\"\n",
    "    test_name = f'test{num}'\n",
    "    print(os.path.join(model_dir, test_name, file_name))\n",
    "    print(os.path.join(model_dir, test_name, stats_fname))\n",
    "    preds = model_predicts[num-single_range[0]]\n",
    "    df = pd.concat([pred_[single_model] for pred_ in preds], axis=1, ignore_index=True).round(precision)\n",
    "    df.to_csv(os.path.join(model_dir, test_name, file_name))\n",
    "    stats_df = []\n",
    "    for p in preds:\n",
    "        p_iter = p.drop('y', axis=1).iteritems()\n",
    "        s = {pred_[0]: calc_statistics(preds[0].y, pred_[1]) for pred_ in p_iter}\n",
    "        stats_df.append(pd.DataFrame.from_dict(s, orient='index'))\n",
    "    stats_df = pd.DataFrame([run.stack() for run in stats_df]).T\n",
    "    stats_df.to_csv(os.path.join(model_dir, test_name, stats_fname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f63232-80b9-4e9d-aec9-7272d77e295f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFMC",
   "language": "python",
   "name": "lfmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
