{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Within-site run times\n",
    "Version of the within-site ablation tests that gets the run-times for the final model and architecture ablation tests.\n",
    "#### Notes\n",
    "- Only one fold model is created for each run\n",
    "- Only 20 runs per test - i.e. timings are for one ensemble of twenty runs.\n",
    "- No parallel running of tests\n",
    "- The first test (test0) is the proposed model, so other tests are offset by 1 compared to the full ablation test (e.g. the dropout test is test4 here and test3 in the ablation test)\n",
    "- By default, training times are output to the `train_stats.csv` file, so these could also be obtained from the ablation tests model directories. This notebook allows testing under more controlled conditions - e.g. if the full runs are run in a shared environment they may not be accurate/consistent due to the server workload. So this is a cut-down version that can run in a small dedicated environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import initialise\n",
    "import common\n",
    "from model_utils import reshape_data\n",
    "from modelling_functions import create_models, run_experiment\n",
    "from architecture_within_site import model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories and Input files\n",
    "Change these settings as required\n",
    "- `modis_csv`: The file containing extracted MODIS data for each sample, created by `Extract MODIS Data.ipynb`\n",
    "- `prism_csv`: The file containing extracted PRISM data for each sample, created by `Extract PRISM Data.ipynb`\n",
    "- `aux_csv`: The file containing extracted sample labels, DEM, climate zone and other auxiliary data, created by `Extract Auxiliary Data.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_csv = os.path.join(common.DATASETS_DIR, 'modis_365days.csv')\n",
    "prism_csv = os.path.join(common.DATASETS_DIR, 'prism_365days.csv')\n",
    "aux_csv = os.path.join(common.DATASETS_DIR, 'samples_365days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up experiment parameters\n",
    "If the experiment dictionary contains a 'tests' key that is not 'falsy' (False, None, 0, empty list) it is assumed to be a list of tests to run. Each test will run with the specified model parameters. Model parameters not specified will be the same for each test, as set in the main model_params dictionary. A failed run can be restarted by setting the 'restart' key to the test that failed. This test and the remaining tests will then be run.\n",
    "\n",
    "If 'tests' is 'falsy' then a single test will be run using the parameters in the main model_params dictionary.\n",
    "\n",
    "Other settings are:\n",
    "- layerTypes: specifies which layers to include in the model\n",
    "- Layer parameters should be specified as a list. The first entry in the list will be used for the first layer, etc.\n",
    "- If the experiment includes changes to the layers, all non-default layer parameters need to be included. The parameters that are kept constant can be specified by including a key for the layer type in the experiment dictionary, and the value set to a dictionary of the constant parameters.\n",
    "\n",
    "Model_parameters that cannot be changed in tests are:\n",
    "- \\*Filename\n",
    "- \\*Channels\n",
    "- targetColumn\n",
    "\n",
    "Example of setting layer parameters:\n",
    "```\n",
    "{'name': 'Filters',\n",
    " 'description': 'Test effect of different filter sizes on conv layers',\n",
    " 'tests': [{'conv': {'filters': [32, 32, 32]}},\n",
    "           {'conv': {'filters': [8, 8, 8]}},\n",
    "           {'conv': {'filters': [32, 8, 8]}},\n",
    "           {'conv': {'filters': [8, 32, 8]}},\n",
    "           {'conv': {'filters': [8, 8, 32]}},\n",
    "           {'conv': {'filters': [8, 16, 32]}},\n",
    "           {'conv': {'filters': [32, 16, 8]}}],\n",
    " 'conv': {'numLayers': 3, 'poolSize': [2, 3, 4]},\n",
    " 'restart': 0}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = {\n",
    "    'name': 'within_site_timings',\n",
    "    'description': 'Timings for within-site architecture changes',\n",
    "    'layerTypes': ['modisConv', 'prismConv', 'fc'],\n",
    "    'tests': [\n",
    "        {},\n",
    "        {'modisConv': {'numLayers': 5, 'filters': [32] * 5, 'poolSize': [0, 5, 2, 3, 4]},\n",
    "         'prismConv': {'numLayers': 5, 'filters': [32] * 5, 'poolSize': [0, 5, 2, 3, 4]}\n",
    "        },\n",
    "        {'modisConv': {'numLayers': 3, 'filters': [8, 8, 8], 'poolSize': [2, 3, 4]},\n",
    "         'prismConv': {'numLayers': 3, 'filters': [8, 8, 8], 'poolSize': [2, 3, 4]}\n",
    "        },\n",
    "        {'fc': {'numLayers': 2, 'units': [512, 512]}},\n",
    "        {'fc': {'numLayers': 3, 'units': [256, 256, 256]}},\n",
    "        {'dropoutRate': 0.5},\n",
    "        {'batchSize': 32},\n",
    "    ],\n",
    "    'restart': None,\n",
    "    'testNames': [\n",
    "        'Proposed model',\n",
    "        'Conv filters: 32',\n",
    "        'Conv layers: 3',\n",
    "        'Dense layers: 2',\n",
    "        'Dense units: 256',\n",
    "        'Dropout: 0.5',\n",
    "        'Batch size: 32',\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save and display experiment details\n",
    "experiment_dir = os.path.join(common.MODELS_DIR, experiment['name'])\n",
    "restart = experiment.get('restart')\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "elif restart is None:\n",
    "    raise FileExistsError(f'{experiment_dir} exists but restart not requested')\n",
    "experiment_file = f'experiment{restart}.json' if restart else 'experiment.json'\n",
    "with open(os.path.join(experiment_dir, experiment_file), 'w') as f:\n",
    "    json.dump(experiment, f, indent=2)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters settings\n",
    "To find out more about any parameter, run `model_params.help('<parameter>')`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_params['modelName'] = experiment['name']\n",
    "model_params['description'] = experiment['description']\n",
    "model_params['modisFilename'] = modis_csv\n",
    "model_params['prismFilename'] = prism_csv\n",
    "model_params['auxFilename'] = aux_csv\n",
    "model_params['splitYear'] = 2017\n",
    "model_params['splitFolds'] = 1\n",
    "model_params['tempDir'] = common.TEMP_DIR\n",
    "model_params['modelDir'] = os.path.join(common.MODELS_DIR, model_params['modelName'])\n",
    "model_params['derivedModels'] = common.DERIVED_MODELS\n",
    "model_params['seedList'] = [\n",
    "    441, 780, 328, 718, 184, 372, 346, 363, 701, 358,\n",
    "    566, 451, 795, 237, 788, 185, 397, 530, 758, 633,\n",
    "    632, 941, 641, 519, 162, 215, 578, 919, 917, 585,\n",
    "    914, 326, 334, 366, 336, 413, 111, 599, 416, 230,\n",
    "    191, 700, 697, 332, 910, 331, 771, 539, 575, 457\n",
    "]\n",
    "\n",
    "if not os.path.exists(model_params['modelDir']):\n",
    "    os.makedirs(model_params['modelDir'])\n",
    "\n",
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_data = pd.read_csv(model_params['modisFilename'], index_col=0)\n",
    "x_modis = reshape_data(np.array(modis_data), model_params['modisChannels'])\n",
    "print(f'Modis shape: {x_modis.shape}')\n",
    "\n",
    "prism_data = pd.read_csv(model_params['prismFilename'], index_col=0)\n",
    "x_prism = reshape_data(np.array(prism_data), model_params['prismChannels'])\n",
    "print(f'Prism shape: {x_prism.shape}')\n",
    "\n",
    "aux_data = pd.read_csv(model_params['auxFilename'], index_col=0)\n",
    "y = aux_data[model_params['targetColumn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the models\n",
    "Builds and trains the LFMC models.\n",
    "\n",
    "All models, predictions, evaluation statistics, and plots of test results are saved to `model_dir`, with each test and run saved to a separate sub-directory. For each model created, predictions and evaluation statistics are also returned as attributes of the `model` object. These are stored as nested lists, the structure for a full experiment is:\n",
    "- Tests (omitted if not an experiment)\n",
    "  - Runs (omitted for a single run)\n",
    "    - Folds (for k-fold splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_experiment():\n",
    "    try:\n",
    "        return bool(experiment['tests'])\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = {'modis': x_modis, 'prism': x_prism}\n",
    "if is_experiment():\n",
    "    ex_models = run_experiment(experiment, model_params, aux_data, X, y)\n",
    "else:\n",
    "    print('Running a single test')\n",
    "    with open(os.path.join(model_params['modelDir'], 'model_params.json'), 'w') as f:\n",
    "        model_params.save(f)\n",
    "    models = create_models(model_params, aux_data, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the training times\n",
    "Time are Tensorflow/Keras model training time and excludes data preparation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_times = []\n",
    "for test in ex_models:\n",
    "    run_time = 0\n",
    "    for run in test:\n",
    "        df = pd.read_csv(os.path.join(run.model_dir, 'train_stats.csv'))\n",
    "        run_time += df.trainTime[0]\n",
    "    weights = df.trainableWeights[0]\n",
    "    train_times.append([run_time/60, run_time/60/len(test), weights])\n",
    "pd.DataFrame(train_times, index=experiment['testNames'],\n",
    "             columns=['ensemble_time', 'single_time', 'num_params']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFMC",
   "language": "python",
   "name": "lfmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
