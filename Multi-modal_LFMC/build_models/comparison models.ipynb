{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFMC Estimation - Comparison Models\n",
    "Runs the comparison tests\n",
    "1. Train the out-of-site architecture using the within-site scenario\n",
    "2. Train the within-site model architecture the out-of-site scenario\n",
    "3. Train the Modis-tempCNN architecture using the within-site scenario\n",
    "4. Train the Modis-tempCNN architecture using the out-of-site scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import initialise\n",
    "import common\n",
    "from model_utils import reshape_data\n",
    "from modelling_functions import create_models, run_experiment\n",
    "from model_parameters import ModelParams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directories and Input files\n",
    "Change these settings as required\n",
    "- `modis_csv`: The file containing extracted MODIS data for each sample, created by `Extract MODIS Data.ipynb`\n",
    "- `prism_csv`: The file containing extracted PRISM data for each sample, created by `Extract PRISM Data.ipynb`\n",
    "- `aux_csv`: The file containing extracted sample labels, DEM, climate zone and other auxiliary data, created by `Extract Auxiliary Data.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_csv = os.path.join(common.DATASETS_DIR, 'modis_365days.csv')\n",
    "prism_csv = os.path.join(common.DATASETS_DIR, 'prism_365days.csv')\n",
    "aux_csv = os.path.join(common.DATASETS_DIR, 'samples_365days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up experiment parameters\n",
    "If the experiment dictionary contains a 'tests' key that is not 'falsy' (False, None, 0, empty list) it is assumed to be a list of tests to run. Each test will run with the specified model parameters. Model parameters not specified will be the same for each test, as set in the main model_params dictionary. A failed run can be restarted by setting the 'restart' key to the test that failed. This test and the remaining tests will then be run.\n",
    "\n",
    "If 'tests' is 'falsy' then a single test will be run using the parameters in the main model_params dictionary.\n",
    "\n",
    "Other settings are:\n",
    "- layerTypes: specifies which layers to include in the model\n",
    "- Layer parameters should be specified as a list. The first entry in the list will be used for the first layer, etc.\n",
    "- If the experiment includes changes to the layers, all non-default layer parameters need to be included. The parameters that are kept constant can be specified by including a key for the layer type in the experiment dictionary, and the value set to a dictionary of the constant parameters.\n",
    "\n",
    "Model_parameters that cannot be changed in tests are:\n",
    "- \\*Filename\n",
    "- \\*Channels\n",
    "- targetColumn\n",
    "\n",
    "Example of setting layer parameters:  \n",
    "```\n",
    "{'name': 'Filters',  \n",
    " 'description': 'Test effect of different filter sizes on conv layers',  \n",
    " 'tests': [{'conv': {'filters': [32, 32, 32]}},  \n",
    "           {'conv': {'filters': [8, 8, 8]}},  \n",
    "           {'conv': {'filters': [32, 8, 8]}},  \n",
    "           {'conv': {'filters': [8, 32, 8]}},   \n",
    "           {'conv': {'filters': [8, 8, 32]}},  \n",
    "           {'conv': {'filters': [8, 16, 32]}},  \n",
    "           {'conv': {'filters': [32, 16, 8]}}],  \n",
    " 'conv': {'numLayers': 3, 'poolSize': [2, 3, 4]},  \n",
    " 'restart': 0}\n",
    " ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "As this experiment uses different architectures for each test, the notebook doesn't import the model architecture parameters. Instead, model architecture parameters are set in each test in the experiment parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment = {\n",
    "    'name': 'comparison_models',\n",
    "    'description': 'Generate comparison models for scenarios using the \"wrong\" and the Modis-tempCNN architecture',\n",
    "    'layerTypes': ['modisConv', 'prismConv', 'fc'],\n",
    "    'tests': [\n",
    "        {'dataSources': ['modis', 'prism', 'aux'], 'batchSize': 512, 'dropoutRate': 0, 'epochs': 50,\n",
    "         'splitMethod': 'byYear', 'splitFolds': 4, 'splitYear': 2014,\n",
    "         'auxColumns': ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm'],\n",
    "         'auxOneHotCols': ['Czone3'], 'auxAugment': True,\n",
    "         'fc': {'numLayers': 1, 'units': [128]},\n",
    "         'modisConv': {'numLayers': 3, 'filters': [8, 8, 8], 'poolSize': [2, 3, 4]},\n",
    "         'prismConv': {'numLayers': 3, 'filters': [8, 8, 8], 'poolSize': [2, 3, 4]}\n",
    "        },\n",
    "        {'dataSources': ['modis', 'prism', 'aux'], 'batchSize': 512, 'dropoutRate': 0.1, 'epochs': 100,\n",
    "         'splitMethod': 'bySite', 'splitFolds': 10,\n",
    "         'auxColumns': ['Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm'],\n",
    "         'auxOneHotCols': ['Czone3'], 'auxAugment': True,\n",
    "         'fc': {'numLayers': 3, 'units': [512, 512, 512]},\n",
    "         'modisConv': {'numLayers': 5, 'filters': [8, 8, 8, 8, 8], 'poolSize': [0, 5, 2, 3, 4]},\n",
    "         'prismConv': {'numLayers': 5, 'filters': [8, 8, 8, 8, 8], 'poolSize': [0, 5, 2, 3, 4]}\n",
    "        },\n",
    "        {'dataSources': ['modis', 'aux'], 'batchSize': 32, 'dropoutRate': 0.5, 'epochs': 100,\n",
    "         'splitMethod': 'byYear', 'splitFolds': 4, 'splitYear': 2014,\n",
    "         'auxColumns': ['Day_sin', 'Day_cos', 'Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm'], \n",
    "         'auxOneHotCols': [], 'auxAugment': False,\n",
    "         'fc': {'numLayers': 2, 'units': [256, 256]},\n",
    "         'modisConv': {'numLayers': 3, 'filters': [32, 32, 32], 'poolSize': [2, 3, 4]},\n",
    "        },\n",
    "        {'dataSources': ['modis', 'aux'], 'batchSize': 32, 'dropoutRate': 0.5, 'epochs': 100,\n",
    "         'splitMethod': 'bySite', 'splitFolds': 10,\n",
    "         'auxColumns': ['Day_sin', 'Day_cos', 'Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos', 'Long_sin', 'Long_cos', 'Lat_norm'], \n",
    "         'auxOneHotCols': [], 'auxAugment': False,\n",
    "         'fc': {'numLayers': 2, 'units': [256, 256]},\n",
    "         'modisConv': {'numLayers': 3, 'filters': [32, 32, 32], 'poolSize': [2, 3, 4]},\n",
    "        },\n",
    "    ],\n",
    "    'restart': None,\n",
    "    'testNames': [\n",
    "        'Out-of-site model within-site scenario',\n",
    "        'Within-site model out-of-site scenario',\n",
    "        'Modis-tempCNN within-site scenario',\n",
    "        'Modis-tempCNN out-of-site scenario',\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save and display experiment details\n",
    "experiment_dir = os.path.join(common.MODELS_DIR, experiment['name'])\n",
    "restart = experiment.get('restart')\n",
    "if not os.path.exists(experiment_dir):\n",
    "    os.makedirs(experiment_dir)\n",
    "elif not restart:\n",
    "    raise FileExistsError(f'{experiment_dir} exists but restart not requested')\n",
    "experiment_file = f'experiment{restart}.json' if restart else 'experiment.json'\n",
    "with open(os.path.join(experiment_dir, experiment_file), 'w') as f:\n",
    "    json.dump(experiment, f, indent=2)\n",
    "experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model parameters\n",
    "Set up and customise the model parameters. To find out more about any parameter, run `model_params.help('<parameter>')` after running this cell to create the ModelParams object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customize model parameters\n",
    "model_params = ModelParams()\n",
    "\n",
    "model_params['modelName'] = experiment['name']\n",
    "model_params['description'] = experiment['description']\n",
    "model_params['modelClass'] = 'LfmcTempCnn'\n",
    "model_params['modisFilename'] = modis_csv\n",
    "model_params['prismFilename'] = prism_csv\n",
    "model_params['auxFilename'] = aux_csv\n",
    "model_params['modelRuns'] = common.EVALUATION_RUNS\n",
    "model_params['seedList'] = [\n",
    "    441, 780, 328, 718, 184, 372, 346, 363, 701, 358,\n",
    "    566, 451, 795, 237, 788, 185, 397, 530, 758, 633,\n",
    "    632, 941, 641, 519, 162, 215, 578, 919, 917, 585,\n",
    "    914, 326, 334, 366, 336, 413, 111, 599, 416, 230,\n",
    "    191, 700, 697, 332, 910, 331, 771, 539, 575, 457\n",
    "]\n",
    "\n",
    "model_params['tempDir'] = common.TEMP_DIR\n",
    "model_params['modelDir'] = os.path.join(common.MODELS_DIR, model_params['modelName'])\n",
    "\n",
    "# =============================================================================\n",
    "# Parameters for parallel execution on GPUs\n",
    "# =============================================================================\n",
    "# model_params['gpuDevice'] = 1\n",
    "# model_params['gpuMemory'] = 3800\n",
    "# model_params['maxWorkers'] = 5\n",
    "\n",
    "if not os.path.exists(model_params['modelDir']):\n",
    "    os.makedirs(model_params['modelDir'])\n",
    "    \n",
    "model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_data = pd.read_csv(model_params['modisFilename'], index_col=0)\n",
    "x_modis = reshape_data(np.array(modis_data), model_params['modisChannels'])\n",
    "print(f'Modis shape: {x_modis.shape}')\n",
    "\n",
    "prism_data = pd.read_csv(model_params['prismFilename'], index_col=0)\n",
    "x_prism = reshape_data(np.array(prism_data), model_params['prismChannels'])\n",
    "print(f'Prism shape: {x_prism.shape}')\n",
    "\n",
    "aux_data = pd.read_csv(model_params['auxFilename'], index_col=0)\n",
    "y = aux_data[model_params['targetColumn']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the models\n",
    "Builds and trains the LFMC models. \n",
    "\n",
    "All models, predictions, and evaluation statisticsare saved to `model_dir`, with each test and run saved to a separate sub-directory. For each model created, predictions and evaluation statistics are also returned as attributes of the `model` object. These are stored as nested lists, the structure for a full experiment is:\n",
    "- Tests (omitted if not an experiment)\n",
    "  - Runs (omitted for a single run)\n",
    "    - Folds (for k-fold splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_experiment():\n",
    "    try:\n",
    "        return bool(experiment['tests'])\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = {'modis': x_modis, 'prism': x_prism}\n",
    "if is_experiment():\n",
    "    models = run_experiment(experiment, model_params, aux_data, X, y)\n",
    "else:\n",
    "    print('Running a single test')\n",
    "    with open(os.path.join(model_params['modelDir'], 'model_params.json'), 'w') as f:\n",
    "        model_params.save(f)\n",
    "    models = create_models(model_params, aux_data, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if is_experiment():\n",
    "    for model in models:\n",
    "        display(getattr(model, 'all_stats', None))\n",
    "else:\n",
    "    display(getattr(models, 'all_stats', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFMC",
   "language": "python",
   "name": "lfmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
