{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b26aa2",
   "metadata": {},
   "source": [
    "# Generate CONUS LFMC Image\n",
    "#### Description\n",
    "Creates a GeoTiff image of LFMC predictions that can be used to produce LFMC maps.\n",
    "\n",
    "#### Input Images\n",
    "1. An image of auxiliary data - latitude, longitude, elevation, slope, aspect and climate zone\n",
    "2. Images of MODIS data for at least 1 year prior to the mapping date\n",
    "3. Images of PRISM data for at least 1 year prior to the mapping date  \n",
    "Note: Band names for PRISM data are confusing. When GEE converts an image collection to an image, it includes the date in the band name. Timestamps on PRISM data are midday, so when they are converted (rounded) to a date, this becomes the following day. So in the extracted GeoTiffs, bands for 2016-10-01 data will named for 2016-10-02!\n",
    "\n",
    "#### Other Inputs\n",
    "1. Model directory - this should contain \"run\" directories - one for each model in the ensemble.\n",
    "2. Data used to train the model - The notebook extracts normalisation bounds and one-hot encodings needed to prepare the input data. The normalisation bounds are saved to csv files, so if these files already exist, bounds can be loaded from these instead.\n",
    "3. Legend file for Koppen climate zones. This should be a CSV as created by the \"Extract Auxiliary Data.ipynb\" notebook. Used to convert the climate zone numbers in the auxiliary input into climate zone codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d62b4295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "christian-infrared",
   "metadata": {},
   "outputs": [],
   "source": [
    "import initialise\n",
    "import common\n",
    "from model_list import ModelList\n",
    "from model_parameters import ModelParams\n",
    "from model_utils import reshape_data, normalise, get_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82073ab9-059e-432c-8df9-3848c9c24ab7",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "The next two cells set up the parameters for a specific map. Uncomment and run the cell for the required map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64cb8d47-d81e-49a8-87d6-50f1b8cc70a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multi-tempCNN Out-of-site Map\n",
    "# LFMC_MODEL = 'Multi-tempCNN'\n",
    "# LFMC_SCENARIO = 'Out-of-Site'\n",
    "# DERIVED_MODEL = common.ANALYSIS_MODEL\n",
    "# MODEL_DIR = os.path.join(common.MODELS_DIR, 'out-of-site_map2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b517b744-303b-4c41-a65f-5d2c16caa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modis-tempCNN Map\n",
    "LFMC_MODEL = 'Modis-tempCNN'\n",
    "LFMC_SCENARIO = '2017'\n",
    "DERIVED_MODEL = common.MODIS_TEMPCNN_MODEL\n",
    "MODEL_DIR = os.path.join(common.MODELS_DIR, 'Modis-tempCNN_map2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "silent-cookie",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAP_DATE = '2017-10-01'\n",
    "MAP_YEAR = MAP_DATE[:4]\n",
    "IMAGE_DIR = 'GEE_EPSG-4326_2000'\n",
    "TS_DAYS = 365\n",
    "TS_OFFSET = 1\n",
    "INTERP_METHOD = 'linear'\n",
    "INTERP_MAXGAP = None\n",
    "INTERP_DIRECTION = 'both'\n",
    "NODATA = -999\n",
    "FLOAT_PRE = 5\n",
    "FN_DATE_POS = (-14, -4)\n",
    "FN_DATE_FORMAT = '%Y-%m-%d'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778dc4ee-ca30-4fb2-b21f-42dcfd6a2479",
   "metadata": {},
   "source": [
    "### Directories and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "conscious-apple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G:\\\\My Drive\\\\LFMC Data\\\\multi_modal_LFMC\\\\Maps data\\\\LFMC_maps\\\\Modis-tempCNN_2017_merge10_2017-10-01.tif'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KOPPEN_LEGEND = os.path.join(common.SOURCE_DIR, 'Climate_zones.csv')\n",
    "OUTPUT_FILE = os.path.join(common.MAPS_DIR, 'LFMC_maps', f'{LFMC_MODEL}_{LFMC_SCENARIO}_{DERIVED_MODEL}_{MAP_DATE}.tif')\n",
    "OUTPUT_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intelligent-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, 'model_params.json'), 'r') as f:\n",
    "    model_params = ModelParams(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bca7be-5191-4cdf-af34-6f0b375b2489",
   "metadata": {},
   "source": [
    "### Load the models\n",
    "Use the \"base\" model and ensemble the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58ef3ee4-0d67-45f1-82cf-a99850003277",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading single model\n",
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "models = ModelList.load_model_set(MODEL_DIR)\n",
    "if isinstance(models, ModelList):\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"Loading model {i}\")\n",
    "        model.load_model(DERIVED_MODEL)\n",
    "else:\n",
    "    print(f\"Loading single model\")\n",
    "    models = [models]\n",
    "    models[0].load_model(DERIVED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d8376-0f53-42c3-a8fc-2a9a9edd57d8",
   "metadata": {},
   "source": [
    "### Import gdal\n",
    "This needs to be done after loading the models, otherwise importing Tensorflow fails!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f7a0694-2e0b-48ec-b936-3a6b2a10073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-allah",
   "metadata": {},
   "source": [
    "## Get Transformation parameters\n",
    "The transformation parameters need to be obtained from the training data, so the mapping data is transformed the same way\n",
    "- Get the normalisation ranges from the MODIS and PRISM data\n",
    "- Get the one-hot encodings from the auxiliary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "based-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_train_file = os.path.join(common.DATASETS_DIR, 'modis_365days.csv')\n",
    "prism_train_file = os.path.join(common.DATASETS_DIR, 'prism_365days.csv')\n",
    "aux_train_file = os.path.join(common.DATASETS_DIR, 'samples_365days.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4d1a28-83d0-453d-b228-30f950b1d47d",
   "metadata": {},
   "source": [
    "### Generate normalisation bounds\n",
    "Generate upper and lower percentiles needed to normalise MODIS and PRISM data from the training data, and save bounds.\n",
    "\n",
    "Note: if bounds have been previously generates, skip the next two cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "general-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal = model_params['modisNormalise']\n",
    "# if normal['method'] == 'minMax':\n",
    "#     modis = pd.read_csv(modis_train_file, index_col=0)\n",
    "#     modis = reshape_data(np.array(modis), model_params['modisChannels'])\n",
    "#     modis_bounds = get_bounds(modis, normal['percentiles'])\n",
    "#     del modis\n",
    "# else:\n",
    "#     modis_bounds = normal['percentiles']\n",
    "# np.savetxt(os.path.join(common.DATASETS_DIR, 'modis_bounds.csv'), modis_bounds, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df870937-e992-495b-976e-a6ff3743ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal = model_params['prismNormalise']\n",
    "# if normal['method'] == 'minMax':\n",
    "#     prism = pd.read_csv(prism_train_file, index_col=0)\n",
    "#     prism = reshape_data(np.array(prism), model_params['prismChannels'])\n",
    "#     prism_bounds = get_bounds(prism, normal['percentiles'])\n",
    "#     del prism\n",
    "# else:\n",
    "#     prism_bounds = normal['percentiles']\n",
    "# np.savetxt(os.path.join(common.DATASETS_DIR, 'prism_bounds.csv'), prism_bounds, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5323199f-9280-4143-aba9-c7602c9b6e6f",
   "metadata": {},
   "source": [
    "### Load previously generated bounds\n",
    "Run this step to load save bounds, if these already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64bbcab3-9446-4100-b9a3-c63e3438da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modis_bounds = np.genfromtxt(os.path.join(common.DATASETS_DIR, 'modis_bounds.csv'), delimiter=',')\n",
    "prism_bounds = np.genfromtxt(os.path.join(common.DATASETS_DIR, 'prism_bounds.csv'), delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf89b876-d4a6-4faf-83c5-48f1f8002d1b",
   "metadata": {},
   "source": [
    "### Auxiliary One-hot Encoder\n",
    "Create the one-hot encoder from the training auxiliary data and encode the climate zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "musical-comfort",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auxiliaries: ['Day_sin', 'Day_cos', 'Long_sin', 'Long_cos', 'Lat_norm', 'Elevation', 'Slope', 'Aspect_sin', 'Aspect_cos']\n"
     ]
    }
   ],
   "source": [
    "samples = pd.read_csv(aux_train_file, index_col=0)\n",
    "if model_params['auxOneHotCols']:\n",
    "    print('One-hot encoding:', model_params['auxOneHotCols'])\n",
    "    onehot_enc = OneHotEncoder(handle_unknown='ignore', sparse=False, dtype='int')\n",
    "    onehot_enc.fit(samples[model_params['auxOneHotCols']].to_numpy())\n",
    "if isinstance(model_params['auxColumns'], int):\n",
    "    model_params['auxColumns'] = list(samples.columns[-model_params['auxColumns']:])\n",
    "print('Auxiliaries:', model_params['auxColumns'])\n",
    "del samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503d311-d687-47a9-903a-2a537777b7b4",
   "metadata": {},
   "source": [
    "Create climate zone conversion table - Assumes:\n",
    "- One-hot encoding includes climate zone\n",
    "- Full 3-level climate zones are wanted\n",
    "- Column name is Czone3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ef58fcb-5898-4515-9ed9-a120f4ff9b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_params['auxOneHotCols']:\n",
    "    czones = pd.read_csv(KOPPEN_LEGEND, index_col=0)\n",
    "    czones_enc = pd.DataFrame(\n",
    "        onehot_enc.transform(czones[['Code']].to_numpy()),\n",
    "        index=czones.index,\n",
    "        columns=onehot_enc.get_feature_names(['Czone']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe72b6a8-82fc-42dd-ac38-a96c481aafdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare the mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ba5022-6834-4862-b9c0-1fae5b94a7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_between(date1, date2):\n",
    "    return (datetime.strptime(date2, FN_DATE_FORMAT) - datetime.strptime(date1, FN_DATE_FORMAT)).days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41695a7e-d170-430e-9452-73b0a3b25ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_after(fn, date_):\n",
    "    return fn[FN_DATE_POS[0]:FN_DATE_POS[1]] > date_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf7706d-e334-413a-84ca-babe06d296ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_files(file_list, start_date, end_date):\n",
    "    for i, fn in enumerate(file_list):\n",
    "        if file_after(fn, start_date):\n",
    "            break\n",
    "    i = i - 1 if i > 0 else 0\n",
    "    keep_last = True\n",
    "    for j, fn in enumerate(file_list[i:], i):\n",
    "        if file_after(fn, end_date):\n",
    "            keep_last = False\n",
    "            break\n",
    "    j = j + 1 if keep_last else j\n",
    "    return file_list[i:j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec12443-83ae-436f-a6ff-9e97a3aeead4",
   "metadata": {},
   "source": [
    "#### Function to prepare auxiliary data\n",
    "- Reads the auxiliary data into a dataframe with column names set to band descriptions. Adds normalised longitude and latitude (unnormalised values retained for referencing/alignment. Replaces elevation, slope, and aspect with normalized values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a8a820c-6577-44dd-9f78-6c06b8abe6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aux_data(aux_image, bands, offsets, sizes):\n",
    "    aux_data = aux_image.ReadAsArray(xoff=offsets[0], yoff=offsets[1], xsize=sizes[0], ysize=sizes[1]).round(FLOAT_PRE)\n",
    "    aux_data = aux_data.transpose(1, 2, 0).reshape(aux_data.shape[1] * aux_data.shape[2], aux_data.shape[0])\n",
    "    aux_df = pd.DataFrame(aux_data, columns=bands)\n",
    "    aux_df = aux_df.replace(aux_image.GetRasterBand(1).GetNoDataValue(), np.NAN).dropna()\n",
    "    if model_params['auxOneHotCols']:\n",
    "        aux_df = aux_df.merge(czones_enc, left_on='climate_zone', right_index=True)\n",
    "    aux_norm = aux_df.drop(['elevation', 'slope', 'aspect', 'climate_zone'], axis=1)\n",
    "    doy = datetime.strptime(MAP_DATE, FN_DATE_FORMAT).timetuple().tm_yday\n",
    "    doy = normalise(doy, method='range', range=(1, 366), out_range=(-np.pi, np.pi))\n",
    "    aux_norm[\"Day_sin\"] = round(np.sin(doy), FLOAT_PRE)\n",
    "    aux_norm[\"Day_cos\"] = round(np.cos(doy), FLOAT_PRE)\n",
    "    longitude = normalise(aux_df.longitude, method='range', range=(-180, 180), out_range=(-np.pi, np.pi))\n",
    "    aux_norm[\"Long_sin\"] = longitude.transform(np.sin).round(FLOAT_PRE)\n",
    "    aux_norm[\"Long_cos\"] = longitude.transform(np.cos).round(FLOAT_PRE)\n",
    "    aux_norm[\"Lat_norm\"] = normalise(aux_df.latitude, method='range', range=(-90, 90)).round(FLOAT_PRE)\n",
    "    aux_norm[\"Elevation\"] = normalise(aux_df.elevation.round(0), method='range', range=(0, 6000)).round(FLOAT_PRE)\n",
    "    aux_norm[\"Slope\"] = normalise(aux_df.slope.round(0), method='range', range=(0, 90)).round(FLOAT_PRE)\n",
    "    aspect = normalise(aux_df.aspect.round(0), method='range', range=(0, 360), out_range=(-np.pi, np.pi))\n",
    "    aux_norm[\"Aspect_sin\"] = aspect.transform(np.sin).round(FLOAT_PRE)\n",
    "    aux_norm[\"Aspect_cos\"] = aspect.transform(np.cos).round(FLOAT_PRE)\n",
    "    return aux_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fe70b5-218c-4f3e-bd01-a9ad782a1b10",
   "metadata": {},
   "source": [
    "#### Function to compute number of pixels between two locations\n",
    "- Input locations can be single values or list-like, but should have the same dimensions\n",
    "- Pixel_size can be a single value, or a value for each pair of location elements\n",
    "- Return value has the same shape as the input locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d092cae-5273-43e0-8a01-94207da8684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_pixels(start_loc, end_loc, pixel_size, convert=np.round):\n",
    "    return_type = type(start_loc)\n",
    "    pixels = (np.array(end_loc) - np.array(start_loc)) / np.array(pixel_size)\n",
    "    if convert:\n",
    "        pixels = convert(pixels)\n",
    "    return return_type(pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c225b312-0341-4201-8697-2b636d6315ce",
   "metadata": {},
   "source": [
    "#### Function to prepare time-series data\n",
    "- Reads data from all images (assumes they are in date order and first image start on ts_start)\n",
    "- Extracts data for the relevant days and pixels\n",
    "  - index parameter indicates required pixels\n",
    "- Interpolates along day axis to fill missing values\n",
    "- Normalises the data using the bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "238e9e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ts_data(images, offsets, sizes, channels, ts_start, start_date, index, bounds, out_range):\n",
    "    ts_data = []\n",
    "    for image in images:\n",
    "        ts_data.append(image.ReadAsArray(xoff=offsets[0], yoff=offsets[1], xsize=sizes[0], ysize=sizes[1]))\n",
    "    start = channels * days_between(ts_start, start_date)\n",
    "    end = start + channels * TS_DAYS\n",
    "    new_shape = (ts_data[0].shape[1] * ts_data[0].shape[2], end - start)\n",
    "    ts_data = np.concatenate(ts_data, axis=0)[start:end].transpose((1, 2, 0)).reshape(new_shape)[index]\n",
    "    ts_data = reshape_data(ts_data, channels)\n",
    "    df = []\n",
    "    for b in range(channels):\n",
    "        df.append(pd.DataFrame(ts_data[:, :, b]).interpolate(axis=1, method=INTERP_METHOD, limit=INTERP_MAXGAP, limit_direction=INTERP_DIRECTION))\n",
    "    ts_data = np.stack(df, axis=-1)\n",
    "    return normalise(ts_data, method='range', range=bounds, out_range=out_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effb8c3-3d11-44c0-a612-044ffb616c33",
   "metadata": {},
   "source": [
    "#### Function to predict LFMC\n",
    "- Models is a ModelList, with trained model loaded\n",
    "- Returns a series indexed by index parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43e6ebd0-25c7-4cb9-b407-8d47024a4ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(models, derived_model, x_aux, x_modis, x_prism, index):\n",
    "    preds = []\n",
    "    X = {'modis': x_modis, 'prism': x_prism, 'aux': x_aux}\n",
    "    start_time = time.time()\n",
    "    for num, model in enumerate(models):\n",
    "        preds.append(model.predict(X, derived_model))\n",
    "    pred_time = round(time.time() - start_time, 2)\n",
    "    print(f'Prediction time:', pred_time)\n",
    "    preds = pd.DataFrame(preds, columns=aux_df.index)\n",
    "    return [preds.mean(axis=0), preds.std(axis=0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96877d3-acb6-403c-b2e0-4fcac39c317d",
   "metadata": {},
   "source": [
    "### Create VRTs\n",
    "Mosaic multiple images for the same date into a VRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13f188ca-4182-4e49-b856-53d1f5830c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mosaic_images(prefix):\n",
    "    file_list = sorted(glob.glob(prefix + \"*.tif\"))\n",
    "    days = sorted(list({fn[len(prefix) : len(prefix) + (FN_DATE_POS[1] - FN_DATE_POS[0])] for fn in file_list}))\n",
    "    for day in days:\n",
    "        print(prefix, day)\n",
    "        gdal.BuildVRT(prefix + day + \".vrt\", glob.glob(prefix + day + \"*.tif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5fa7ab5-fe59-40e5-ad72-803daee86fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\My Drive\\LFMC Data\\Maps data\\GEE_EPSG-4326_2000\\MODIS_ 2016-04-01\n",
      "G:\\My Drive\\LFMC Data\\Maps data\\GEE_EPSG-4326_2000\\MODIS_ 2016-07-01\n",
      "G:\\My Drive\\LFMC Data\\Maps data\\GEE_EPSG-4326_2000\\MODIS_ 2016-10-01\n",
      "G:\\My Drive\\LFMC Data\\Maps data\\GEE_EPSG-4326_2000\\MODIS_ 2017-01-01\n",
      "G:\\My Drive\\LFMC Data\\Maps data\\GEE_EPSG-4326_2000\\MODIS_ 2017-04-01\n",
      "G:\\My Drive\\LFMC Data\\Maps data\\GEE_EPSG-4326_2000\\MODIS_ 2017-07-01\n"
     ]
    }
   ],
   "source": [
    "mosaic_images(os.path.join(common.GEE_MAPS_DIR, IMAGE_DIR, f'MODIS_'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5768e-8235-4ee8-8127-ddd268d24817",
   "metadata": {},
   "source": [
    "### Open Image Files\n",
    "- Allows for multiple MODIS and PRISM images\n",
    "- Assumes MODIS and PRISM image file names include first date in image in file name\n",
    "- Assumes images are contiguous with no overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb4a8a51-6558-436a-9263-805afd16b2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_file = os.path.join(common.GEE_MAPS_DIR, IMAGE_DIR, f'conus_aux.tif')\n",
    "aux_image = gdal.Open(aux_file, gdal.GA_ReadOnly)\n",
    "bands = []\n",
    "for b in range(1, aux_image.RasterCount+1):\n",
    "    bands.append(aux_image.GetRasterBand(b).GetDescription())\n",
    "    \n",
    "end_date = datetime.strptime(MAP_DATE, FN_DATE_FORMAT) - timedelta(TS_OFFSET)\n",
    "start_date = str((end_date - timedelta(TS_DAYS-1)).date())\n",
    "end_date = str(end_date.date())\n",
    "\n",
    "modis_files = sorted(glob.glob(os.path.join(common.GEE_MAPS_DIR, IMAGE_DIR, f'MODIS_*.vrt')))\n",
    "modis_files = select_files(modis_files, start_date, end_date)\n",
    "modis_images = [gdal.Open(fn, gdal.GA_ReadOnly) for fn in modis_files]\n",
    "\n",
    "origin = (modis_images[0].GetGeoTransform()[0], modis_images[0].GetGeoTransform()[3])\n",
    "pixel_size = (modis_images[0].GetGeoTransform()[1], modis_images[0].GetGeoTransform()[5])\n",
    "raster_size = (modis_images[0].RasterXSize, modis_images[0].RasterYSize)\n",
    "origin_aux = num_pixels((aux_image.GetGeoTransform()[0], aux_image.GetGeoTransform()[3]),\n",
    "                        origin, pixel_size)\n",
    "# Process by block - assumes all images and bands have the same block size\n",
    "batch_size = aux_image.GetRasterBand(1).GetBlockSize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7568f-b4ee-490c-bfa3-c1090b15aabf",
   "metadata": {},
   "source": [
    "#### Convert PRISM data to mapping resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cccbde08-d658-4070-92ff-0f6d446807ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'prism' in model_params['dataSources']:\n",
    "    prism_files = sorted(glob.glob(os.path.join(common.GEE_MAPS_DIR, IMAGE_DIR, f'PRISM_*.tif')))\n",
    "    prism_files = select_files(prism_files, start_date, end_date)\n",
    "    prism_images = [gdal.Open(fn, gdal.GA_ReadOnly) for fn in prism_files]\n",
    "\n",
    "    # MODIS data projection and resolution\n",
    "    proj = modis_images[0].GetProjection()\n",
    "    geotrans = modis_images[0].GetGeoTransform()\n",
    "    x_size = modis_images[0].RasterXSize\n",
    "    y_size = modis_images[0].RasterYSize\n",
    "\n",
    "    for num in range(len(prism_images)):\n",
    "        # In-memory raster for the reprojected data\n",
    "        print(prism_files[num])\n",
    "        dst = gdal.GetDriverByName('MEM').Create(\"\", x_size, y_size, prism_images[num].RasterCount, gdal.GDT_Float32)\n",
    "        dst.SetGeoTransform(geotrans)\n",
    "        dst.SetProjection(proj)\n",
    "        gdal.ReprojectImage(prism_images[num], dst, proj, proj, gdal.GRA_NearestNeighbour)\n",
    "        prism_images[num] = dst\n",
    "        \n",
    "    origin_prism = num_pixels((prism_images[0].GetGeoTransform()[0], prism_images[0].GetGeoTransform()[3]),\n",
    "                              origin, pixel_size)\n",
    "    print(origin, origin_aux, origin_prism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed67ca2-94f1-4032-aaba-8d52f60cdaeb",
   "metadata": {},
   "source": [
    "### Create output Geotiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c48bc45-c101-4dac-a953-e28831ac0dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver = gdal.GetDriverByName('GTiff')\n",
    "out_map_raster = driver.Create(OUTPUT_FILE, modis_images[0].RasterXSize, modis_images[0].RasterYSize, 2, gdal.GDT_Float32)\n",
    "out_map_raster.SetGeoTransform(modis_images[0].GetGeoTransform())\n",
    "out_map_raster.SetProjection(modis_images[0].GetProjectionRef())\n",
    "lfmc_band = out_map_raster.GetRasterBand(1)\n",
    "lfmc_band.SetNoDataValue(NODATA)\n",
    "std_band = out_map_raster.GetRasterBand(2)\n",
    "std_band.SetNoDataValue(NODATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a6d7a7-96df-4d10-a472-007369208865",
   "metadata": {},
   "source": [
    "## Generate LFMC estimates\n",
    "Loop through the images by raster block, prepare the data, make LFMC predictions and save to output raster.\n",
    "- Nodata pixels are removed before making predictions\n",
    "- Indexes used to link between dataframes and arrays\n",
    "- Block processing skipped if all pixels in aux block are nodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ced3627-3144-4f27-9000-f2d73ac3e4bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing block at (0, 0), size (256, 256), 46116 predictions\n",
      "Aux processing: 0.07\n",
      "Modis processing: 6.17\n",
      "Prediction time: 3.75\n",
      "Processing block at (256, 0), size (256, 256), 59388 predictions\n",
      "Aux processing: 0.07\n",
      "Modis processing: 7.49\n",
      "Prediction time: 1.87\n",
      "Processing block at (512, 0), size (256, 256), 59632 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.52\n",
      "Prediction time: 1.77\n",
      "Processing block at (768, 0), size (256, 256), 59763 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.51\n",
      "Prediction time: 1.81\n",
      "Processing block at (1024, 0), size (256, 256), 59634 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.64\n",
      "Prediction time: 1.8\n",
      "Processing block at (1280, 0), size (256, 256), 58727 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.06\n",
      "Prediction time: 1.78\n",
      "Processing block at (1536, 0), size (256, 256), 54907 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 6.74\n",
      "Prediction time: 1.69\n",
      "Processing block at (1792, 0), size (256, 256), 35946 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 4.92\n",
      "Prediction time: 1.35\n",
      "Processing block at (2048, 0), size (256, 256), 14989 predictions\n",
      "Aux processing: 0.03\n",
      "Modis processing: 3.1\n",
      "Prediction time: 0.59\n",
      "Processing block at (2304, 0), size (256, 256), 37 predictions\n",
      "Aux processing: 0.02\n",
      "Modis processing: 1.11\n",
      "Prediction time: 0.18\n",
      "Processing block at (2560, 0), size (256, 256), 642 predictions\n",
      "Aux processing: 0.02\n",
      "Modis processing: 1.1\n",
      "Prediction time: 0.16\n",
      "Processing block at (2816, 0), size (256, 256), 8377 predictions\n",
      "Aux processing: 0.03\n",
      "Modis processing: 1.97\n",
      "Prediction time: 0.33\n",
      "Processing block at (3072, 0), size (145, 256), 14343 predictions\n",
      "Aux processing: 0.02\n",
      "Modis processing: 2.15\n",
      "Prediction time: 0.54\n",
      "Processing block at (0, 256), size (256, 256), 57427 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.09\n",
      "Prediction time: 1.87\n",
      "Processing block at (256, 256), size (256, 256), 65295 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 7.93\n",
      "Prediction time: 2.0\n",
      "Processing block at (512, 256), size (256, 256), 63695 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 7.66\n",
      "Prediction time: 1.9\n",
      "Processing block at (768, 256), size (256, 256), 65112 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 8.0\n",
      "Prediction time: 1.97\n",
      "Processing block at (1024, 256), size (256, 256), 65328 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.75\n",
      "Prediction time: 2.02\n",
      "Processing block at (1280, 256), size (256, 256), 64918 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.96\n",
      "Prediction time: 1.95\n",
      "Processing block at (1536, 256), size (256, 256), 65107 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.84\n",
      "Prediction time: 1.97\n",
      "Processing block at (1792, 256), size (256, 256), 64476 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.83\n",
      "Prediction time: 2.01\n",
      "Processing block at (2048, 256), size (256, 256), 51332 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 6.76\n",
      "Prediction time: 1.86\n",
      "Processing block at (2304, 256), size (256, 256), 28682 predictions\n",
      "Aux processing: 0.03\n",
      "Modis processing: 4.21\n",
      "Prediction time: 0.95\n",
      "Processing block at (2560, 256), size (256, 256), 52619 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 6.59\n",
      "Prediction time: 1.63\n",
      "Processing block at (2816, 256), size (256, 256), 41928 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 5.43\n",
      "Prediction time: 1.36\n",
      "Processing block at (3072, 256), size (145, 256), 3030 predictions\n",
      "Aux processing: 0.02\n",
      "Modis processing: 1.2\n",
      "Prediction time: 0.3\n",
      "Processing block at (0, 512), size (256, 256), 36307 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 4.93\n",
      "Prediction time: 1.14\n",
      "Processing block at (256, 512), size (256, 256), 65016 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.78\n",
      "Prediction time: 1.95\n",
      "Processing block at (512, 512), size (256, 256), 65234 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 7.92\n",
      "Prediction time: 2.0\n",
      "Processing block at (768, 512), size (256, 256), 65400 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.86\n",
      "Prediction time: 1.98\n",
      "Processing block at (1024, 512), size (256, 256), 65430 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.85\n",
      "Prediction time: 2.01\n",
      "Processing block at (1280, 512), size (256, 256), 65392 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.93\n",
      "Prediction time: 2.04\n",
      "Processing block at (1536, 512), size (256, 256), 64708 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 7.92\n",
      "Prediction time: 1.92\n",
      "Processing block at (1792, 512), size (256, 256), 64703 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.91\n",
      "Prediction time: 1.92\n",
      "Processing block at (2048, 512), size (256, 256), 64964 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 8.05\n",
      "Prediction time: 1.93\n",
      "Processing block at (2304, 512), size (256, 256), 65234 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 7.87\n",
      "Prediction time: 1.86\n",
      "Processing block at (2560, 512), size (256, 256), 43536 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 5.67\n",
      "Prediction time: 1.36\n",
      "Processing block at (2816, 512), size (256, 256), 66 predictions\n",
      "Aux processing: 0.02\n",
      "Modis processing: 1.05\n",
      "Prediction time: 0.25\n",
      "Processing block at (3072, 512), size (145, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (0, 768), size (256, 256), 2032 predictions\n",
      "Aux processing: 0.02\n",
      "Modis processing: 1.21\n",
      "Prediction time: 0.26\n",
      "Processing block at (256, 768), size (256, 256), 30407 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 4.19\n",
      "Prediction time: 1.02\n",
      "Processing block at (512, 768), size (256, 256), 50880 predictions\n",
      "Aux processing: 0.04\n",
      "Modis processing: 6.37\n",
      "Prediction time: 1.58\n",
      "Processing block at (768, 768), size (256, 256), 58092 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 7.05\n",
      "Prediction time: 1.81\n",
      "Processing block at (1024, 768), size (256, 256), 64718 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 7.86\n",
      "Prediction time: 1.93\n",
      "Processing block at (1280, 768), size (256, 256), 65226 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 8.48\n",
      "Prediction time: 1.98\n",
      "Processing block at (1536, 768), size (256, 256), 64019 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 8.12\n",
      "Prediction time: 1.91\n",
      "Processing block at (1792, 768), size (256, 256), 64592 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 7.87\n",
      "Prediction time: 2.05\n",
      "Processing block at (2048, 768), size (256, 256), 64896 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 7.88\n",
      "Prediction time: 1.92\n",
      "Processing block at (2304, 768), size (256, 256), 50954 predictions\n",
      "Aux processing: 0.06\n",
      "Modis processing: 6.52\n",
      "Prediction time: 1.58\n",
      "Processing block at (2560, 768), size (256, 256), 8875 predictions\n",
      "Aux processing: 0.03\n",
      "Modis processing: 2.04\n",
      "Prediction time: 0.4\n",
      "Processing block at (2816, 768), size (256, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (3072, 768), size (145, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (0, 1024), size (256, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (256, 1024), size (256, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (512, 1024), size (256, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (768, 1024), size (256, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (1024, 1024), size (256, 256), 14916 predictions\n",
      "Aux processing: 0.03\n",
      "Modis processing: 2.61\n",
      "Prediction time: 0.62\n",
      "Processing block at (1280, 1024), size (256, 256), 46845 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 6.05\n",
      "Prediction time: 1.46\n",
      "Processing block at (1536, 1024), size (256, 256), 25002 predictions\n",
      "Aux processing: 0.05\n",
      "Modis processing: 3.73\n",
      "Prediction time: 0.84\n",
      "Processing block at (1792, 1024), size (256, 256), 15643 predictions\n",
      "Aux processing: 0.03\n",
      "Modis processing: 2.75\n",
      "Prediction time: 0.55\n",
      "Processing block at (2048, 1024), size (256, 256), 12177 predictions\n",
      "Aux processing: 0.03\n",
      "Modis processing: 2.33\n",
      "Prediction time: 0.52\n",
      "Processing block at (2304, 1024), size (256, 256), 27912 predictions\n",
      "Aux processing: 0.03\n",
      "Modis processing: 3.98\n",
      "Prediction time: 0.88\n",
      "Processing block at (2560, 1024), size (256, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (2816, 1024), size (256, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (3072, 1024), size (145, 256), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (0, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (256, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (512, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (768, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (1024, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (1280, 1280), size (256, 103), 1554 predictions\n",
      "Aux processing: 0.01\n",
      "Modis processing: 0.87\n",
      "Prediction time: 0.19\n",
      "Processing block at (1536, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (1792, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (2048, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (2304, 1280), size (256, 103), 4579 predictions\n",
      "Aux processing: 0.01\n",
      "Modis processing: 1.05\n",
      "Prediction time: 0.26\n",
      "Processing block at (2560, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (2816, 1280), size (256, 103), 0 predictions\n",
      "No data in block - skipping\n",
      "Processing block at (3072, 1280), size (145, 103), 0 predictions\n",
      "No data in block - skipping\n"
     ]
    }
   ],
   "source": [
    "modis_start = modis_files[0][FN_DATE_POS[0]:FN_DATE_POS[1]]\n",
    "if 'prism' in model_params['dataSources']:\n",
    "    prism_start = prism_files[0][FN_DATE_POS[0]:FN_DATE_POS[1]]\n",
    "augment = model_params['auxAugment']\n",
    "for y_offset in range(0, raster_size[1], batch_size[1]):\n",
    "    for x_offset in range(0, raster_size[0], batch_size[0]):\n",
    "        start_time = time.time()\n",
    "        x_size = min(batch_size[0], raster_size[0] - x_offset)\n",
    "        y_size = min(batch_size[1], raster_size[1] - y_offset)\n",
    "        lfmc_index = pd.Index(range(y_size * x_size))\n",
    "        aux_df = get_aux_data(aux_image, bands, (int(x_offset + origin_aux[0]), int(y_offset + origin_aux[1])), (x_size, y_size))\n",
    "        print(f'Processing block at ({x_offset}, {y_offset}), size ({x_size}, {y_size}), {len(aux_df)} predictions')\n",
    "        if len(aux_df) > 0:\n",
    "            if model_params['auxOneHotCols']:\n",
    "                x_aux = aux_df[model_params['auxColumns'] + list(czones_enc.columns)].to_numpy()\n",
    "            else:\n",
    "                x_aux = aux_df[model_params['auxColumns']].to_numpy()\n",
    "            aux_time = time.time()\n",
    "            print('Aux processing:', round(aux_time - start_time, 2))\n",
    "            x_modis = None\n",
    "            x_prism = None\n",
    "\n",
    "            for source in model_params['dataSources']:\n",
    "                if source == 'modis':\n",
    "                    x_modis = get_ts_data(\n",
    "                        modis_images,\n",
    "                        (x_offset, y_offset),\n",
    "                        (x_size, y_size),\n",
    "                        model_params['modisChannels'],\n",
    "                        modis_start,\n",
    "                        start_date,\n",
    "                        aux_df.index,\n",
    "                        modis_bounds,\n",
    "                        model_params['modisNormalise'].get('out_range', (0, 1)),\n",
    "                    )\n",
    "                    if (augment is True) or (isinstance(augment, list) and 'modis' in augment):\n",
    "                        x_aux = np.concatenate([x_aux, x_modis[:, -1, :]], axis=1)\n",
    "                    elif isinstance(augment, dict) and 'modis' in augment.keys():\n",
    "                        offset = augment[input_name] or 1\n",
    "                        x_aux = np.concatenate([x_aux, x_modis[:, -offset, :]], axis=1)\n",
    "                    modis_time = time.time()\n",
    "                    print('Modis processing:', round(modis_time - aux_time, 2))\n",
    "\n",
    "                if source == 'prism':\n",
    "                    x_prism = get_ts_data(\n",
    "                        prism_images,\n",
    "                        (int(x_offset + origin_prism[0]), int(y_offset + origin_prism[1])),\n",
    "                        (x_size, y_size),\n",
    "                        model_params['prismChannels'],\n",
    "                        prism_start,\n",
    "                        start_date,\n",
    "                        aux_df.index,\n",
    "                        prism_bounds,\n",
    "                        model_params['prismNormalise'].get('out_range', (0, 1)),\n",
    "                    )\n",
    "                    if (augment is True) or (isinstance(augment, list) and 'prism' in augment):\n",
    "                        x_aux = np.concatenate([x_aux, x_prism[:, -1, :]], axis=1)\n",
    "                    elif isinstance(augment, dict) and 'prism' in augment.keys():\n",
    "                        offset = augment[input_name] or 1\n",
    "                        x_aux = np.concatenate([x_aux, x_prism[:, -offset, :]], axis=1)\n",
    "                    prism_time = time.time()\n",
    "                    print('Prism processing:', round(prism_time - modis_time, 2))\n",
    "\n",
    "            lfmc, std_dev = predict(models, DERIVED_MODEL, x_aux, x_modis, x_prism, aux_df.index)\n",
    "            lfmc = lfmc.reindex(lfmc_index).to_numpy().reshape(1, y_size, x_size)\n",
    "            lfmc[np.isnan(lfmc)] = NODATA\n",
    "            lfmc_band.WriteArray(lfmc[0], xoff=x_offset, yoff=y_offset)\n",
    "            lfmc_band.FlushCache()\n",
    "            std_dev = std_dev.reindex(lfmc_index).to_numpy().reshape(1, y_size, x_size)\n",
    "            std_dev[np.isnan(std_dev)] = NODATA\n",
    "            std_band.WriteArray(std_dev[0], xoff=x_offset, yoff=y_offset)\n",
    "            std_band.FlushCache()\n",
    "        else:\n",
    "            print('No data in block - skipping')\n",
    "            empty = np.full([y_size, x_size], NODATA)\n",
    "            lfmc_band.WriteArray(empty, xoff=x_offset, yoff=y_offset)\n",
    "            lfmc_band.FlushCache()\n",
    "            std_band.WriteArray(empty, xoff=x_offset, yoff=y_offset)\n",
    "            std_band.FlushCache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e77bb90-d57e-4a8b-bb8a-ccd9ffd7cde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del out_map_raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d133d28e-c7de-4253-9ee8-1075b06a8f2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFMC",
   "language": "python",
   "name": "lfmc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
