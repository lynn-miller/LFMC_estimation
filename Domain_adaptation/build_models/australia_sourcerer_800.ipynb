{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LFMC Estimation Experiment\n",
    "Notebook to test LFMC modelling code changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up experiment parameters\n",
    "If the experiment dictionary contains a 'tests' key that is not 'falsy' (False, None, 0, empty list) it is assumed to be a list of tests to run. Each test will run with the specified model parameters. Model parameters not specified will be the same for each test, as set in the main model_params dictionary. A failed run can be restarted by setting the 'restart' key to the test that failed. This test and the remaining tests will then be run.\n",
    "\n",
    "If 'tests' is 'falsy' then a single test will be run using the parameters in the main model_params dictionary.\n",
    "\n",
    "For more help, after running this cell run `experiment.help()` or `experiment.help('<parameter>')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_experiment():\n",
    "    import os\n",
    "    import initialise\n",
    "    import common\n",
    "    experiment = {\n",
    "        'name': 'australia_sourcerer_800',\n",
    "        'description': 'Australia: pretrained on CONUS; Sourcerer, targetMax=800; all training samples',\n",
    "        'tests': [],\n",
    "        'restart': False, \n",
    "        'rerun': None,\n",
    "        'resumeAllTests': False,\n",
    "    } #)\n",
    "    folds_dir = os.path.join(common.MODELS_DIR, 'australia_gen_folds')\n",
    "    pretrained_dir = os.path.join(common.MODELS_DIR, 'conus_base_models')\n",
    "\n",
    "    seeds = [9013, 1815, 5313, 3945, 3632, 3875, 1782, 1393, 3708, 2914,\n",
    "             4522, 3368, 6379, 3009, 3806, 6579, 4075, 1056, 5261, 4752]\n",
    "    for n, s in enumerate(seeds):\n",
    "        experiment['tests'].append({\n",
    "            'testName': f'Ensemble {n+1}', 'randomSeed': s,\n",
    "            'loadFolds': os.path.join(folds_dir, f'test{n}'),\n",
    "            'pretrainedModel': os.path.join(pretrained_dir, f'test{n}')})\n",
    "\n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model parameters\n",
    "Set up and customise the model parameters. Leave all parameters as set here to run Scenario A. To find out more about any parameter, run `model_params.help('<parameter>')` after running this cell to create the ModelParams object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_up_model_params(experiment):\n",
    "    import os\n",
    "    import initialise\n",
    "    import common\n",
    "    from architecture_transfer import model_params\n",
    "    model_params['modelName'] = experiment['name']\n",
    "    model_params['description'] = experiment['description']\n",
    "    model_params['modelRuns'] = 20\n",
    "    model_params['plotModel'] = False\n",
    "    \n",
    "    # Globe-LFMC Column Names\n",
    "    model_params['splitColumn'] = 'Group2'\n",
    "    model_params['yearColumn'] = 'Sampling year'\n",
    "    \n",
    "    # Train/test split parameters\n",
    "    model_params['splitMethod'] = 'byValue'\n",
    "    model_params['splitFolds'] = 4\n",
    "    model_params['splitYear'] = 2014\n",
    "    model_params['yearFolds'] = 3\n",
    "    model_params['splitMax'] = True\n",
    "    model_params['saveFolds'] = True\n",
    "\n",
    "    # Transfer learning parameters\n",
    "    model_params['pretrainedModel'] = os.path.join(common.MODELS_DIR, 'conus_base_models', 'test1')\n",
    "    model_params['transferModel'] = {'method': 'sourcerer', 'targetMax': 800}\n",
    "    model_params['commonNormalise'] = False\n",
    "\n",
    "    # Other parameters\n",
    "    model_params['epochs'] = 1000\n",
    "    model_params['evaluateEpochs'] = 100\n",
    "    model_params['derivedModels'] = None\n",
    "    model_params['seedList'] = [\n",
    "        441, 780, 328, 718, 184, 372, 346, 363, 701, 358,\n",
    "        566, 451, 795, 237, 788, 185, 397, 530, 758, 633,\n",
    "        632, 941, 641, 519, 162, 215, 578, 919, 917, 585,\n",
    "        914, 326, 334, 366, 336, 413, 111, 599, 416, 230,\n",
    "        191, 700, 697, 332, 910, 331, 771, 539, 575, 457\n",
    "    ]\n",
    "    model_params['maxWorkers'] = 24     # Number of workers (parallel processes)\n",
    "    model_params['gpuList'] = [0, 1]    # List of GPUs to use\n",
    "    model_params['gpuMemory'] = 256     # GPU memory for each worker\n",
    "    return model_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the models\n",
    "Builds and trains the LFMC models.\n",
    "\n",
    "All models, predictions, evaluation statistics, and plots of test results are saved to `model_dir`, with each test and run saved to a separate sub-directory. For each model created, predictions and evaluation statistics are also returned as attributes of the `model` object. These are stored as nested lists, the structure for a full experiment is:\n",
    "- Tests (omitted if not an experiment)\n",
    "  - Runs (omitted for a single run)\n",
    "    - Folds (for k-fold splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_job(experiment): #, model_params):\n",
    "    import os\n",
    "    import initialise\n",
    "    import common\n",
    "    from modelling_functions import run_experiment\n",
    "    from model_parameters import ExperimentParams\n",
    "\n",
    "    os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/usr/lib/cuda'\n",
    "\n",
    "    # Input files\n",
    "    modis_csv = os.path.join(common.DATASETS_DIR, 'australia_modis_365days.csv')\n",
    "    era5_csv = os.path.join(common.DATASETS_DIR, 'australia_era5_365days.csv')\n",
    "    aux_csv = os.path.join(common.DATASETS_DIR, 'australia_samples_365days.csv')\n",
    "\n",
    "    # Experiment parameters\n",
    "    experiment = ExperimentParams(experiment)\n",
    "\n",
    "    # Model parameters\n",
    "    model_params = set_up_model_params(experiment)\n",
    "    model_params['tempDir'] = common.TEMP_DIR\n",
    "    model_params['modelDir'] = os.path.join(common.MODELS_DIR, model_params['modelName'])\n",
    "    \n",
    "    # Model inputs\n",
    "    model_params['samplesFile'] = aux_csv\n",
    "    model_params.add_input('optical', {'filename': modis_csv, 'channels': 7})\n",
    "    model_params.add_input('weather', {'filename': era5_csv, 'channels': 7})\n",
    "\n",
    "    models = run_experiment(experiment, model_params)\n",
    "\n",
    "    # Dask doesn't seem to like returning model_list objects, so we can't return the models\n",
    "    return 'Finished'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "\n",
    "tests_per_run = 5\n",
    "experiment = set_up_experiment()\n",
    "first_test = experiment['restart'] or 0\n",
    "num_tests = len(experiment['tests'])\n",
    "experiment['testsPerRun'] = tests_per_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests 0 - 5 result: Finished\n",
      "Tests 5 - 10 result: Finished\n",
      "Tests 10 - 15 result: Finished\n",
      "Tests 15 - 20 result: Finished\n"
     ]
    }
   ],
   "source": [
    "for next in range(first_test, num_tests, tests_per_run):\n",
    "    if next > first_test:\n",
    "        experiment['restart'] = next\n",
    "        \n",
    "    cluster = SLURMCluster(\n",
    "        memory=\"128g\", processes=1, cores=16, job_extra_directives=[\"--gres=gpu:2\"], nanny=False\n",
    "    )\n",
    "    cluster.scale(1)\n",
    "    client = Client(cluster) \n",
    "\n",
    "    future = client.submit(run_job, experiment) #, model_params)\n",
    "    print(f'Tests {next} - {min(next+tests_per_run, num_tests)} result: {future.result()}')\n",
    "\n",
    "    client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
